{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b709e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "import fasttext.util\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "350cb5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_en_train_0 = 'data/train/en/En-Subtask1-fold_0.tsv'\n",
    "path_en_train_1 = 'data/train/en/En-Subtask1-fold_1.tsv'\n",
    "path_en_train_2 = 'data/train/en/En-Subtask1-fold_2.tsv'\n",
    "\n",
    "path_en_test    = 'data/test/En-Subtask1-labels.tsv'\n",
    "\n",
    "path_fr_train_0 = 'data/train/fr/Fr-Subtask1-fold_0.tsv'\n",
    "path_fr_train_1 = 'data/train/fr/Fr-Subtask1-fold_1.tsv'\n",
    "path_fr_train_2 = 'data/train/fr/Fr-Subtask1-fold_2.tsv'\n",
    "\n",
    "path_fr_test    = 'data/test/Fr-Subtask1-labels.tsv'\n",
    "\n",
    "path_it_train_0 = 'data/train/it/It-Subtask1-fold_0.tsv'\n",
    "path_it_train_1 = 'data/train/it/It-Subtask1-fold_1.tsv'\n",
    "path_it_train_2 = 'data/train/it/It-Subtask1-fold_2.tsv'\n",
    "\n",
    "path_it_test    = 'data/test/It-Subtask1-labels.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b367299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data:\n",
    "\n",
    "#EN\n",
    "#Train data \n",
    "train_data_en_0 = pd.read_table(path_en_train_0)\n",
    "train_data_en_1 = pd.read_table(path_en_train_1)\n",
    "train_data_en_2 = pd.read_table(path_en_train_2)\n",
    "#Test data\n",
    "test_data_en    = pd.read_table(path_en_test)\n",
    "\n",
    "\n",
    "#FR\n",
    "#Train data \n",
    "train_data_fr_0 = pd.read_table(path_fr_train_0)\n",
    "train_data_fr_1 = pd.read_table(path_fr_train_1)\n",
    "train_data_fr_2 = pd.read_table(path_fr_train_2)\n",
    "#Test data\n",
    "test_data_fr    = pd.read_table(path_fr_test)\n",
    "\n",
    "\n",
    "#IT\n",
    "#Train data \n",
    "train_data_it_0 = pd.read_table(path_it_train_0)\n",
    "train_data_it_1 = pd.read_table(path_it_train_1)\n",
    "train_data_it_2 = pd.read_table(path_it_train_2)\n",
    "#Test data\n",
    "test_data_it    = pd.read_table(path_it_test)\n",
    "\n",
    "#Define sentences and labels arrays:\n",
    "\n",
    "#EN-----------------------------------------------------------\n",
    "#Train data \n",
    "\n",
    "#Sentences:\n",
    "train_sentences_en_0 = train_data_en_0[['Sentence']].to_numpy()\n",
    "train_sentences_en_1 = train_data_en_1[['Sentence']].to_numpy()\n",
    "train_sentences_en_2 = train_data_en_2[['Sentence']].to_numpy()\n",
    "\n",
    "#Labels\n",
    "train_labels_en_0 = train_data_en_0['Labels'].to_numpy()\n",
    "train_labels_en_1 = train_data_en_1['Labels'].to_numpy()\n",
    "train_labels_en_2 = train_data_en_2['Labels'].to_numpy()\n",
    "\n",
    "#Test data:\n",
    "test_sentences_en = test_data_en[['Sentence']].to_numpy()\n",
    "test_labels_en    = test_data_en['Labels'].to_numpy()\n",
    "\n",
    "\n",
    "#FR-----------------------------------------------------------\n",
    "#Train data \n",
    "\n",
    "#Sentences:\n",
    "train_sentences_fr_0 = train_data_fr_0[['Sentence']].to_numpy()\n",
    "train_sentences_fr_1 = train_data_fr_1[['Sentence']].to_numpy()\n",
    "train_sentences_fr_2 = train_data_fr_2[['Sentence']].to_numpy()\n",
    "\n",
    "#Labels\n",
    "train_labels_fr_0 = train_data_fr_0['Labels'].to_numpy()\n",
    "train_labels_fr_1 = train_data_fr_1['Labels'].to_numpy()\n",
    "train_labels_fr_2 = train_data_fr_2['Labels'].to_numpy()\n",
    "\n",
    "#Test data:\n",
    "test_sentences_fr = test_data_fr[['Sentence']].to_numpy()\n",
    "test_labels_fr    = test_data_fr['Labels'].to_numpy()\n",
    "\n",
    "\n",
    "#IT-----------------------------------------------------------\n",
    "#Train data \n",
    "\n",
    "#Senetnces:\n",
    "train_sentences_it_0 = train_data_it_0[['Sentence']].to_numpy()\n",
    "train_sentences_it_1 = train_data_it_1[['Sentence']].to_numpy()\n",
    "train_sentences_it_2 = train_data_it_2[['Sentence']].to_numpy()\n",
    "\n",
    "#Labels\n",
    "train_labels_it_0 = train_data_it_0['Labels'].to_numpy()\n",
    "train_labels_it_1 = train_data_it_1['Labels'].to_numpy()\n",
    "train_labels_it_2 = train_data_it_2['Labels'].to_numpy()\n",
    "\n",
    "#Test data:\n",
    "test_sentences_it = test_data_it[['Sentence']].to_numpy()\n",
    "test_labels_it    = test_data_it['Labels'].to_numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc9ec2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get tokenizers, define tokenization function \n",
    "tokenizer_en = get_tokenizer('spacy', language=\"en_core_web_sm\")\n",
    "tokenizer_fr = get_tokenizer('spacy', language=\"fr_core_news_sm\")\n",
    "tokenizer_it = get_tokenizer('spacy', language=\"it_core_news_sm\")\n",
    "\n",
    "def tokenize(data,tokenizer):\n",
    "    tokenised_sentences = []\n",
    "    for row in data:\n",
    "        tokenised_sentences.append(tokenizer(row[0]))\n",
    "    return tokenised_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6896a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EN---------------------------\n",
    "tokenized_train_sentences_en_0 = tokenize(train_sentences_en_0, tokenizer_en)\n",
    "tokenized_train_sentences_en_1 = tokenize(train_sentences_en_1, tokenizer_en)\n",
    "tokenized_train_sentences_en_2 = tokenize(train_sentences_en_2, tokenizer_en)\n",
    "\n",
    "tokenized_test_sentences_en    = tokenize(test_sentences_en,tokenizer_en)\n",
    "\n",
    "#EN---------------------------\n",
    "tokenized_train_sentences_fr_0 = tokenize(train_sentences_fr_0,tokenizer_fr)\n",
    "tokenized_train_sentences_fr_1 = tokenize(train_sentences_fr_1,tokenizer_fr)\n",
    "tokenized_train_sentences_fr_2 = tokenize(train_sentences_fr_2,tokenizer_fr)\n",
    "\n",
    "tokenized_test_sentences_fr    = tokenize(test_sentences_fr, tokenizer_fr)\n",
    "\n",
    "#IT---------------------------\n",
    "tokenized_train_sentences_it_0 = tokenize(train_sentences_it_0, tokenizer_it)\n",
    "tokenized_train_sentences_it_1 = tokenize(train_sentences_it_1, tokenizer_it)\n",
    "tokenized_train_sentences_it_2 = tokenize(train_sentences_it_2, tokenizer_it)\n",
    "\n",
    "tokenized_test_sentences_it    = tokenize(test_sentences_it, tokenizer_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f02c6d",
   "metadata": {},
   "source": [
    "##### Load embedding vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "146f85da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load embeddings 100 \n",
    "ft_en_100 = fasttext.load_model('pretrained_embeddings/cc.en.100.bin')\n",
    "\n",
    "#Load embeddings 300 \n",
    "ft_en_300 = fasttext.load_model('pretrained_embeddings/cc.en.300.bin')\n",
    "\n",
    "#Load embeddings 100 FR\n",
    "ft_fr_100 = fasttext.load_model('pretrained_embeddings/cc.fr.100.bin')\n",
    "\n",
    "#Load embeddings 300 FR\n",
    "ft_fr_300 = fasttext.load_model('pretrained_embeddings/cc.fr.300.bin')\n",
    "\n",
    "#Load embeddings 100 IT\n",
    "ft_it_100 = fasttext.load_model('pretrained_embeddings/cc.it.100.bin')\n",
    "\n",
    "#Load embeddings 300 IT\n",
    "ft_it_300 = fasttext.load_model('pretrained_embeddings/cc.it.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46702b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27372fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace words in sentences with their corresponding vector\n",
    "def vectorize(sentences, ft):\n",
    "    sentences_vectorised = []\n",
    "    for s in sentences:\n",
    "        sentence_vectorised = []\n",
    "        for word in s:\n",
    "            sentence_vectorised.append(ft.get_word_vector(word))\n",
    "        sentences_vectorised.append(sentence_vectorised)\n",
    "    return sentences_vectorised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbf34a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EN-----------------------------\n",
    "vectorized_train_sentences_en_0 = vectorize(tokenized_train_sentences_en_0, ft_en_100)\n",
    "vectorized_train_sentences_en_1 = vectorize(tokenized_train_sentences_en_1, ft_en_100)\n",
    "vectorized_train_sentences_en_2 = vectorize(tokenized_train_sentences_en_2, ft_en_100)\n",
    "\n",
    "vectorized_test_sentences_en    = vectorize(tokenized_test_sentences_en, ft_en_100)\n",
    "\n",
    "#EN-----------------------------\n",
    "vectorized_train_sentences_fr_0 = vectorize(tokenized_train_sentences_fr_0, ft_fr_100)\n",
    "vectorized_train_sentences_fr_1 = vectorize(tokenized_train_sentences_fr_1, ft_fr_100)\n",
    "vectorized_train_sentences_fr_2 = vectorize(tokenized_train_sentences_fr_2, ft_fr_100)\n",
    "vectorized_test_sentences_fr    = vectorize(tokenized_test_sentences_fr, ft_fr_100)\n",
    "\n",
    "#EN-----------------------------\n",
    "vectorized_train_sentences_it_0 = vectorize(tokenized_train_sentences_it_0, ft_it_100)\n",
    "vectorized_train_sentences_it_1 = vectorize(tokenized_train_sentences_it_1, ft_it_100)\n",
    "vectorized_train_sentences_it_2 = vectorize(tokenized_train_sentences_it_2, ft_it_100)\n",
    "\n",
    "vectorized_test_sentences_it    = vectorize(tokenized_test_sentences_it, ft_it_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b5ab0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding function:\n",
    "#Adapt dataset so that the sentence vectors are all of the same length \n",
    "def pad_vectorized_sentences(sentences, size):\n",
    "    #Find out the dimension of the word vector -- we need to pad the sentence with 0 vectors of this dimension\n",
    "    embedding_length = len(sentences[0][0]) # length first word vector of the first sentence of the dataset\n",
    "    X = []\n",
    "    i =0\n",
    "    for s in sentences:\n",
    "        result = np.zeros((size, embedding_length)) \n",
    "        s = np.array(s)\n",
    "        result[:s.shape[0],:s.shape[1]] = s\n",
    "        X.append(result)\n",
    "        i = i+1\n",
    "    return np.array(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2db67b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 18  # for english 15 was chosen -- better performance\n",
    "#EN----------------\n",
    "X_train_padded_en_0 = pad_vectorized_sentences(vectorized_train_sentences_en_0, 15)\n",
    "X_train_padded_en_1 = pad_vectorized_sentences(vectorized_train_sentences_en_1, 15)\n",
    "X_train_padded_en_2 = pad_vectorized_sentences(vectorized_train_sentences_en_2, 15)\n",
    "\n",
    "X_test_padded_en  = pad_vectorized_sentences(vectorized_test_sentences_en, 15)\n",
    "\n",
    "#FR----------------\n",
    "X_train_padded_fr_0 = pad_vectorized_sentences(vectorized_train_sentences_fr_0, size)\n",
    "X_train_padded_fr_1 = pad_vectorized_sentences(vectorized_train_sentences_fr_1, size)\n",
    "X_train_padded_fr_2 = pad_vectorized_sentences(vectorized_train_sentences_fr_2, size)\n",
    "\n",
    "X_test_padded_fr  = pad_vectorized_sentences(vectorized_test_sentences_fr, size)\n",
    "\n",
    "#IT----------------\n",
    "X_train_padded_it_0 = pad_vectorized_sentences(vectorized_train_sentences_it_0, size)\n",
    "X_train_padded_it_1 = pad_vectorized_sentences(vectorized_train_sentences_it_1, size)\n",
    "X_train_padded_it_2 = pad_vectorized_sentences(vectorized_train_sentences_it_2, size)\n",
    "\n",
    "X_test_padded_it  = pad_vectorized_sentences(vectorized_test_sentences_it, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a848d3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2091a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to tensors (datatype: float32)\n",
    "\n",
    "#EN-------------------------\n",
    "X_train_en_0 = torch.tensor(X_train_padded_en_0.astype('float32'))\n",
    "X_train_en_1 = torch.tensor(X_train_padded_en_1.astype('float32'))\n",
    "X_train_en_2 = torch.tensor(X_train_padded_en_2.astype('float32'))\n",
    "\n",
    "Y_train_en_0   = torch.tensor(train_labels_en_0)\n",
    "Y_train_en_1   = torch.tensor(train_labels_en_1)\n",
    "Y_train_en_2   = torch.tensor(train_labels_en_2)\n",
    "\n",
    "#Testing dataset\n",
    "X_test_en = torch.tensor(X_test_padded_en.astype('float32'))\n",
    "Y_test_en = torch.tensor(test_labels_en)\n",
    "\n",
    "\n",
    "#FR-------------------------\n",
    "X_train_fr_0 = torch.tensor(X_train_padded_fr_0.astype('float32'))\n",
    "X_train_fr_1 = torch.tensor(X_train_padded_fr_1.astype('float32'))\n",
    "X_train_fr_2 = torch.tensor(X_train_padded_fr_2.astype('float32'))\n",
    "\n",
    "Y_train_fr_0   = torch.tensor(train_labels_fr_0)\n",
    "Y_train_fr_1   = torch.tensor(train_labels_fr_1)\n",
    "Y_train_fr_2   = torch.tensor(train_labels_fr_2)\n",
    "\n",
    "#Testing dataset\n",
    "X_test_fr = torch.tensor(X_test_padded_fr.astype('float32'))\n",
    "Y_test_fr = torch.tensor(test_labels_fr)\n",
    "\n",
    "\n",
    "#FR-------------------------\n",
    "X_train_it_0 = torch.tensor(X_train_padded_it_0.astype('float32'))\n",
    "X_train_it_1 = torch.tensor(X_train_padded_it_1.astype('float32'))\n",
    "X_train_it_2 = torch.tensor(X_train_padded_it_2.astype('float32'))\n",
    "\n",
    "Y_train_it_0   = torch.tensor(train_labels_it_0)\n",
    "Y_train_it_1   = torch.tensor(train_labels_it_1)\n",
    "Y_train_it_2   = torch.tensor(train_labels_it_2)\n",
    "\n",
    "#Testing dataset\n",
    "X_test_it = torch.tensor(X_test_padded_it.astype('float32'))\n",
    "Y_test_it = torch.tensor(test_labels_it)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c860033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a974c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataset tensors in a file, for other future usages:\n",
    "\n",
    "#EN----------------------------------\n",
    "torch.save(X_train_en_0,'tensors/x_train_en_0_file.pt') \n",
    "torch.save(X_train_en_1,'tensors/x_train_en_1_file.pt')\n",
    "torch.save(X_train_en_2,'tensors/x_train_en_2_file.pt')\n",
    "torch.save(Y_train_en_0,'tensors/y_train_en_0_file.pt')\n",
    "torch.save(Y_train_en_1,'tensors/y_train_en_1_file.pt')\n",
    "torch.save(Y_train_en_2,'tensors/y_train_en_2_file.pt')\n",
    "\n",
    "#Testing dataset\n",
    "torch.save(X_test_en,'tensors/x_test_en_file.pt')\n",
    "torch.save(Y_test_en,'tensors/y_test_en_file.pt')\n",
    "\n",
    "#EN-----------------------------------\n",
    "torch.save(X_train_fr_0,'tensors/x_train_fr_0_file.pt') \n",
    "torch.save(X_train_fr_1,'tensors/x_train_fr_1_file.pt')\n",
    "torch.save(X_train_fr_2,'tensors/x_train_fr_2_file.pt')\n",
    "torch.save(Y_train_fr_0,'tensors/y_train_fr_0_file.pt')\n",
    "torch.save(Y_train_fr_1,'tensors/y_train_fr_1_file.pt')\n",
    "torch.save(Y_train_fr_2,'tensors/y_train_fr_2_file.pt')\n",
    "\n",
    "#Testing dataset\n",
    "torch.save(X_test_fr,'tensors/x_test_fr_file.pt')\n",
    "torch.save(Y_test_fr,'tensors/y_test_fr_file.pt')\n",
    "\n",
    "#EN-----------------------------------\n",
    "torch.save(X_train_it_0,'tensors/x_train_it_0_file.pt') \n",
    "torch.save(X_train_it_1,'tensors/x_train_it_1_file.pt')\n",
    "torch.save(X_train_it_2,'tensors/x_train_it_2_file.pt')\n",
    "torch.save(Y_train_it_0,'tensors/y_train_it_0_file.pt')\n",
    "torch.save(Y_train_it_1,'tensors/y_train_it_1_file.pt')\n",
    "torch.save(Y_train_it_2,'tensors/y_train_it_2_file.pt')\n",
    "\n",
    "#Testing dataset\n",
    "torch.save(X_test_it,'tensors/x_test_it_file.pt')\n",
    "torch.save(Y_test_it,'tensors/y_test_it_file.pt')\n",
    "\n",
    "#loaded_tesnsor = torch.load('y_test_file.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2373a8a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7f5c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenation: merge all the folds \n",
    "\n",
    "X_train_en = torch.cat((X_train_en_0,X_train_en_1,X_train_en_2),0)\n",
    "X_train_fr = torch.cat((X_train_fr_0,X_train_fr_1,X_train_fr_2),0)\n",
    "X_train_it = torch.cat((X_train_it_0,X_train_it_1,X_train_it_2),0)\n",
    "\n",
    "Y_train_en = torch.cat((Y_train_en_0,Y_train_en_1,Y_train_en_2),0)\n",
    "Y_train_fr = torch.cat((Y_train_fr_0,Y_train_fr_1,Y_train_fr_2),0)\n",
    "Y_train_it = torch.cat((Y_train_it_0,Y_train_it_1,Y_train_it_2),0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f57b045",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "100e9e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, n_layers):\n",
    "       \n",
    "        super(LSTM,self).__init__()\n",
    "        \n",
    "        self.input_size  = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers    = n_layers\n",
    "        \n",
    "        #LSTM layer\n",
    "        self.lstm_layer = nn.LSTM(input_size, hidden_size, n_layers,batch_first=True)\n",
    "        #LINEAR layer\n",
    "        self.output_layer = nn.Linear(hidden_size,1)\n",
    "    \n",
    "    def forward(self, input_sentence, hidden_state ):\n",
    "        \n",
    "        sig = nn.Sigmoid()\n",
    "      \n",
    "        output, state = self.lstm_layer(input_sentence, hidden_state ) \n",
    "     \n",
    "        output_network = sig(self.output_layer(state[0]).view(1))\n",
    "        \n",
    "        return(output_network, state)\n",
    "\n",
    "    def init_hidden(self):\n",
    "      \n",
    "        return (torch.zeros(self.n_layers, 1, self.hidden_size),\n",
    "                torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c1184f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, input_s, label):\n",
    "    hidden, cell = model.init_hidden()\n",
    "    model.zero_grad()\n",
    "    loss = 0\n",
    "    output, (hidden, cell) = model(input_s, (hidden, cell))\n",
    "    loss += criterion(output, label.view(1).to(torch.float32))   \n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56aac450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The train and time logging function were taken from assignment 3 of the course\n",
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac68e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dd1d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and run model: \n",
    "\n",
    "def create_model(X,Y, hidden_size, optimizer_key, n_layers, loss_function, lr, n_epochs):\n",
    "    \n",
    "    input_size = X.size()[2] \n",
    "    seq_len    = X.size()[1] \n",
    "    \n",
    "    model = LSTM(input_size, hidden_size, n_layers)\n",
    "    \n",
    "    criterion = loss_function\n",
    "    \n",
    "    start = time.time()\n",
    "    all_losses = []\n",
    "    loss_avg = 0\n",
    "    optimizer = 0\n",
    "    \n",
    "    if optimizer_key == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    elif optimizer_key == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_key == 'RMSprop':\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    elif optimizer_key == 'Adagrad':\n",
    "        optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)\n",
    "\n",
    "    \n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        for i in range(X.size()[0]): \n",
    "            \n",
    "            loss = train(model,optimizer, X[i].view(1, seq_len,input_size ),Y[i])\n",
    "            loss_avg += loss\n",
    "\n",
    "        print('[{} ({} {}%) {:.4f}]'.format(time_since(start), epoch, epoch/n_epochs * 100, loss))\n",
    "    \n",
    "\n",
    "    all_losses.append(loss_avg/ n_epochs)\n",
    "    loss_avg = 0\n",
    "    return model, all_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f958a639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(model,X_test,Y_test):\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    \n",
    "    hidden, cell = model.init_hidden()\n",
    "    logits  = []\n",
    "    labels  = []\n",
    "    seq_len = X_test.size()[1] \n",
    "    input_size = X_test.size()[2] \n",
    "    for i in range(X_test.size()[0]):\n",
    "        \n",
    "        gg, (hidden,cell) = model(X_test[i].view(1,seq_len,input_size ),(hidden, cell))\n",
    "        \n",
    "        if(gg[0]>=0.5):\n",
    "            logits.append(1) \n",
    "        else:\n",
    "            logits.append(0)   \n",
    "            \n",
    "        labels.append(Y_test[i])\n",
    "    \n",
    "    correct = 0\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(logits)):\n",
    "        labl = labels[i]\n",
    "        pred = logits[i]\n",
    "        if (labl==pred):\n",
    "            if labl == 1:\n",
    "                tp += 1\n",
    "            else: \n",
    "                tn += 1\n",
    "        else: \n",
    "            if labl == 1:\n",
    "                fn += 1\n",
    "            else: \n",
    "                fp += 1\n",
    "            \n",
    "    accuracy = (tp+tn)/len(labels)\n",
    "    precision = (tp)/(tp+fp)\n",
    "    recall = (tp)/(tp+fn)\n",
    "    f1 = 2*tp/(2*tp+fp+fn)\n",
    "    \n",
    "    return (accuracy,precision,recall,f1)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ade04",
   "metadata": {},
   "source": [
    "###  EXPERIMETS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c4eb7",
   "metadata": {},
   "source": [
    "## ENGLISH \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a446e570",
   "metadata": {},
   "source": [
    "### Fold 0, base config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bf73d340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 11s (1 12.5%) 0.5793]\n",
      "[0m 23s (2 25.0%) 0.5794]\n",
      "[0m 35s (3 37.5%) 0.5819]\n",
      "[0m 47s (4 50.0%) 0.5150]\n",
      "[0m 58s (5 62.5%) 0.2272]\n",
      "[1m 10s (6 75.0%) 0.2085]\n",
      "[1m 22s (7 87.5%) 0.1810]\n",
      "[1m 34s (8 100.0%) 0.1541]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.7822115384615385\n",
      "Precision: 0.826302729528536\n",
      "Recall: 0.6802860061287027\n",
      "F1: 0.746218487394958\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "experiment = \"En 0 base\"\n",
    "n_epochs = 8\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_0\n",
    "Y= Y_train_en_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6487c437",
   "metadata": {},
   "source": [
    "### Fold 1, base config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f232309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 12s (1 100.0%) 0.5685]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.4706730769230769\n",
      "Precision: 0.4706730769230769\n",
      "Recall: 1.0\n",
      "F1: 0.6400784570120954\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "experiment = \"En 1 base\"\n",
    "n_epochs = 1\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_1\n",
    "Y= Y_train_en_1\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0e3a35",
   "metadata": {},
   "source": [
    "### Fold 2, base config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49a3574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 11s (1 12.5%) 0.7869]\n",
      "[0m 23s (2 25.0%) 0.7844]\n",
      "[0m 35s (3 37.5%) 0.7855]\n",
      "[0m 47s (4 50.0%) 0.7919]\n",
      "[0m 59s (5 62.5%) 0.7335]\n",
      "[1m 11s (6 75.0%) 0.3967]\n",
      "[1m 23s (7 87.5%) 0.2159]\n",
      "[1m 34s (8 100.0%) 0.2459]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.7978708791208792\n",
      "Precision: 0.8205969170219745\n",
      "Recall: 0.7301911571574493\n",
      "F1: 0.7727588603196665\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "experiment = \"En 2 base\"\n",
    "n_epochs = 8\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_2\n",
    "Y= Y_train_en_2\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0612429f",
   "metadata": {},
   "source": [
    "### Fold 0, base config + change embedding vector to 300 features (instead of 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4e2c6c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 12s (1 12.5%) 0.5796]\n",
      "[0m 25s (2 25.0%) 0.5825]\n",
      "[0m 38s (3 37.5%) 0.4565]\n",
      "[0m 51s (4 50.0%) 0.1750]\n",
      "[1m 4s (5 62.5%) 0.1803]\n",
      "[1m 17s (6 75.0%) 0.1356]\n",
      "[1m 29s (7 87.5%) 0.1060]\n",
      "[1m 42s (8 100.0%) 0.0960]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.711195054945055\n",
      "Precision: 0.8333333333333334\n",
      "Recall: 0.48300014592149426\n",
      "F1: 0.6115473441108545\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Prepare input data: \n",
    "vectorized_300_train_sentences_en_0 = vectorize(tokenized_train_sentences_en_0, ft_en_300)\n",
    "vectorized_300_test_sentences_en    = vectorize(tokenized_test_sentences_en, ft_en_300)\n",
    "X_train_padded_en_0_300 = pad_vectorized_sentences(vectorized_300_train_sentences_en_0, 15)\n",
    "X_test_padded_en_300  = pad_vectorized_sentences(vectorized_300_test_sentences_en, 15)\n",
    "\n",
    "experiment = \"EN 0 with 300 ft\"\n",
    "n_epochs = 8\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= torch.tensor(X_train_padded_en_0_300.astype('float32'))\n",
    "Y= Y_train_en_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = torch.tensor(X_test_padded_en_300.astype('float32'))\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf1f8fe",
   "metadata": {},
   "source": [
    "### Fold 1, base config + more epochs   (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c4617743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 12s (1 10.0%) 0.5686]\n",
      "[0m 23s (2 20.0%) 0.5688]\n",
      "[0m 35s (3 30.0%) 0.5688]\n",
      "[0m 47s (4 40.0%) 0.5687]\n",
      "[0m 59s (5 50.0%) 0.5683]\n",
      "[1m 11s (6 60.0%) 0.5505]\n",
      "[1m 23s (7 70.0%) 0.1293]\n",
      "[1m 35s (8 80.0%) 0.1081]\n",
      "[1m 46s (9 90.0%) 0.1126]\n",
      "[1m 59s (10 100.0%) 0.1307]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.7760302197802198\n",
      "Precision: 0.7532430908065426\n",
      "Recall: 0.7795126222092514\n",
      "F1: 0.7661527429186088\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#EN Fold 1\n",
    "experiment = \"En fold 1\"\n",
    "n_epochs = 10\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_1\n",
    "Y= Y_train_en_1\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d2774",
   "metadata": {},
   "source": [
    " ### Fold 1, base config + more epochs   (15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37ae7136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 12s (1 6.666666666666667%) 0.5688]\n",
      "[0m 24s (2 13.333333333333334%) 0.5690]\n",
      "[0m 35s (3 20.0%) 0.5690]\n",
      "[0m 47s (4 26.666666666666668%) 0.5690]\n",
      "[0m 59s (5 33.33333333333333%) 0.5639]\n",
      "[1m 11s (6 40.0%) 0.1949]\n",
      "[1m 23s (7 46.666666666666664%) 0.1204]\n",
      "[1m 35s (8 53.333333333333336%) 0.1345]\n",
      "[1m 47s (9 60.0%) 0.1468]\n",
      "[1m 59s (10 66.66666666666666%) 0.1569]\n",
      "[2m 11s (11 73.33333333333333%) 0.1484]\n",
      "[2m 23s (12 80.0%) 0.1610]\n",
      "[2m 35s (13 86.66666666666667%) 0.1703]\n",
      "[2m 46s (14 93.33333333333333%) 0.1850]\n",
      "[2m 58s (15 100.0%) 0.1852]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.792032967032967\n",
      "Precision: 0.8025628856193641\n",
      "Recall: 0.7402597402597403\n",
      "F1: 0.7701533323212388\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#EN Fold 1\n",
    "experiment = \"En fold 1\"\n",
    "n_epochs = 15\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_1\n",
    "Y= Y_train_en_1\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1c89ad",
   "metadata": {},
   "source": [
    "### Fold 1, base config + more epochs   (20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d7ec5430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 11s (1 5.0%) 0.5686]\n",
      "[0m 23s (2 10.0%) 0.5688]\n",
      "[0m 35s (3 15.0%) 0.5688]\n",
      "[0m 47s (4 20.0%) 0.5687]\n",
      "[0m 59s (5 25.0%) 0.5680]\n",
      "[1m 10s (6 30.0%) 0.3491]\n",
      "[1m 22s (7 35.0%) 0.1314]\n",
      "[1m 34s (8 40.0%) 0.1141]\n",
      "[1m 46s (9 45.0%) 0.1238]\n",
      "[1m 58s (10 50.0%) 0.1423]\n",
      "[2m 10s (11 55.00000000000001%) 0.1521]\n",
      "[2m 22s (12 60.0%) 0.1571]\n",
      "[2m 33s (13 65.0%) 0.1693]\n",
      "[2m 45s (14 70.0%) 0.1791]\n",
      "[2m 57s (15 75.0%) 0.1829]\n",
      "[3m 9s (16 80.0%) 0.1783]\n",
      "[3m 21s (17 85.0%) 0.1733]\n",
      "[3m 33s (18 90.0%) 0.1746]\n",
      "[3m 45s (19 95.0%) 0.1723]\n",
      "[3m 57s (20 100.0%) 0.1695]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.779945054945055\n",
      "Precision: 0.8285611381235368\n",
      "Recall: 0.6713847949803006\n",
      "F1: 0.7417378687731743\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#EN Fold 1\n",
    "experiment = \"En fold 1\"\n",
    "n_epochs = 20\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_1\n",
    "Y= Y_train_en_1\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8fb5cf",
   "metadata": {},
   "source": [
    "### Fold 0, base config + num hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "beb9fdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 8s (1 12.5%) 0.5790]\n",
      "[0m 16s (2 25.0%) 0.5790]\n",
      "[0m 24s (3 37.5%) 0.5795]\n",
      "[0m 32s (4 50.0%) 0.5826]\n",
      "[0m 40s (5 62.5%) 0.4859]\n",
      "[0m 47s (6 75.0%) 0.2416]\n",
      "[0m 55s (7 87.5%) 0.1888]\n",
      "[1m 3s (8 100.0%) 0.1670]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.7776785714285714\n",
      "Precision: 0.8021390374331551\n",
      "Recall: 0.7004231723332847\n",
      "F1: 0.7478382799719561\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#EN Fold 0\n",
    "experiment = \"En fold 1\"\n",
    "n_epochs = 8\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_0\n",
    "Y= Y_train_en_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e82d832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 8s (1 6.666666666666667%) 0.5793]\n",
      "[0m 16s (2 13.333333333333334%) 0.5790]\n",
      "[0m 24s (3 20.0%) 0.5796]\n",
      "[0m 32s (4 26.666666666666668%) 0.5868]\n",
      "[0m 40s (5 33.33333333333333%) 0.5274]\n",
      "[0m 48s (6 40.0%) 0.1886]\n",
      "[0m 56s (7 46.666666666666664%) 0.1631]\n",
      "[1m 4s (8 53.333333333333336%) 0.1239]\n",
      "[1m 12s (9 60.0%) 0.1249]\n",
      "[1m 20s (10 66.66666666666666%) 0.1034]\n",
      "[1m 28s (11 73.33333333333333%) 0.1036]\n",
      "[1m 36s (12 80.0%) 0.0944]\n",
      "[1m 43s (13 86.66666666666667%) 0.0974]\n",
      "[1m 51s (14 93.33333333333333%) 0.0920]\n",
      "[1m 59s (15 100.0%) 0.0906]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.7783653846153846\n",
      "Precision: 0.822139303482587\n",
      "Recall: 0.6751787538304392\n",
      "F1: 0.7414469994391475\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#EN Fold 0\n",
    "experiment = \"En fold 1\"\n",
    "n_epochs = 15\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_0\n",
    "Y= Y_train_en_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55200a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 8s (1 4.0%) 0.5795]\n",
      "[0m 15s (2 8.0%) 0.5796]\n",
      "[0m 23s (3 12.0%) 0.5841]\n",
      "[0m 31s (4 16.0%) 0.4900]\n",
      "[0m 39s (5 20.0%) 0.1711]\n",
      "[0m 47s (6 24.0%) 0.2307]\n",
      "[0m 55s (7 28.000000000000004%) 0.1885]\n",
      "[1m 3s (8 32.0%) 0.1419]\n",
      "[1m 11s (9 36.0%) 0.1281]\n",
      "[1m 19s (10 40.0%) 0.1220]\n",
      "[1m 27s (11 44.0%) 0.1152]\n",
      "[1m 35s (12 48.0%) 0.1063]\n",
      "[1m 43s (13 52.0%) 0.0994]\n",
      "[1m 51s (14 56.00000000000001%) 0.0937]\n",
      "[1m 59s (15 60.0%) 0.0904]\n",
      "[2m 6s (16 64.0%) 0.0914]\n",
      "[2m 14s (17 68.0%) 0.0877]\n",
      "[2m 22s (18 72.0%) 0.0850]\n",
      "[2m 30s (19 76.0%) 0.0838]\n",
      "[2m 38s (20 80.0%) 0.0854]\n",
      "[2m 46s (21 84.0%) 0.0850]\n",
      "[2m 54s (22 88.0%) 0.0831]\n",
      "[3m 2s (23 92.0%) 0.0803]\n",
      "[3m 9s (24 96.0%) 0.0768]\n",
      "[3m 17s (25 100.0%) 0.0734]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.7140796703296703\n",
      "Precision: 0.8307919331037875\n",
      "Recall: 0.4929228075295491\n",
      "F1: 0.6187379796684678\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#EN Fold 0\n",
    "experiment = \"En fold 1\"\n",
    "n_epochs = 25\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_0\n",
    "Y= Y_train_en_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5caa1",
   "metadata": {},
   "source": [
    "### Fold 0, base config + learning rate  (0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad7a9946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 11s (1 12.5%) 0.6065]\n",
      "[0m 23s (2 25.0%) 0.6069]\n",
      "[0m 35s (3 37.5%) 0.6095]\n",
      "[0m 46s (4 50.0%) 0.5146]\n",
      "[0m 58s (5 62.5%) 0.2199]\n",
      "[1m 10s (6 75.0%) 0.1214]\n",
      "[1m 21s (7 87.5%) 0.1084]\n",
      "[1m 33s (8 100.0%) 0.0982]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.7864010989010989\n",
      "Precision: 0.8230623165889867\n",
      "Recall: 0.6957536845177295\n",
      "F1: 0.7540724339712161\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#EN Fold 0\n",
    "experiment = \"En fold 1\"\n",
    "n_epochs = 8\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.2\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_0\n",
    "Y= Y_train_en_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4ee955",
   "metadata": {},
   "source": [
    "###  Fold 0, base config + learning rate  (0.2) + n epoch = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c516762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 11s (1 6.666666666666667%) 0.6064]\n",
      "[0m 23s (2 13.333333333333334%) 0.6067]\n",
      "[0m 36s (3 20.0%) 0.6081]\n",
      "[0m 48s (4 26.666666666666668%) 0.6003]\n",
      "[1m 0s (5 33.33333333333333%) 0.3175]\n",
      "[1m 11s (6 40.0%) 0.1977]\n",
      "[1m 23s (7 46.666666666666664%) 0.1351]\n",
      "[1m 35s (8 53.333333333333336%) 0.1093]\n",
      "[1m 46s (9 60.0%) 0.1024]\n",
      "[1m 58s (10 66.66666666666666%) 0.1016]\n",
      "[2m 9s (11 73.33333333333333%) 0.1008]\n",
      "[2m 21s (12 80.0%) 0.1049]\n",
      "[2m 32s (13 86.66666666666667%) 0.1095]\n",
      "[2m 44s (14 93.33333333333333%) 0.1139]\n",
      "[2m 56s (15 100.0%) 0.1099]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.7566620879120879\n",
      "Precision: 0.8306032760687175\n",
      "Recall: 0.6067415730337079\n",
      "F1: 0.7012395648874272\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#EN Fold 0\n",
    "experiment = \"En fold 1\"\n",
    "n_epochs = 15\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.2\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_0\n",
    "Y= Y_train_en_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6b02d2",
   "metadata": {},
   "source": [
    "###  Fold 0, base config + learning rate  (0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71b0f173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 12s (1 12.5%) 0.6303]\n",
      "[0m 24s (2 25.0%) 0.6304]\n",
      "[0m 35s (3 37.5%) 0.6309]\n",
      "[0m 47s (4 50.0%) 0.6118]\n",
      "[0m 58s (5 62.5%) 0.3390]\n",
      "[1m 10s (6 75.0%) 0.1541]\n",
      "[1m 22s (7 87.5%) 0.1216]\n",
      "[1m 34s (8 100.0%) 0.1104]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.778021978021978\n",
      "Precision: 0.7879751868935899\n",
      "Recall: 0.7228950824456443\n",
      "F1: 0.7540334855403349\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#EN Fold 0\n",
    "experiment = \"En fold 1\"\n",
    "n_epochs = 8\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.3\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_0\n",
    "Y= Y_train_en_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c389de3",
   "metadata": {},
   "source": [
    "### More padding: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7eddc1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 15s (1 12.5%) 0.5790]\n",
      "[0m 30s (2 25.0%) 0.5785]\n",
      "[0m 45s (3 37.5%) 0.5784]\n",
      "[1m 0s (4 50.0%) 0.5784]\n",
      "[1m 15s (5 62.5%) 0.5784]\n",
      "[1m 30s (6 75.0%) 0.5784]\n",
      "[1m 45s (7 87.5%) 0.5784]\n",
      "[2m 0s (8 100.0%) 0.5784]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-839dc1ebc126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precision: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-bbd72bb94840>\u001b[0m in \u001b[0;36mmetrics\u001b[0;34m(model, X_test, Y_test)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "X_train_padded_en_0_18 = pad_vectorized_sentences(vectorized_train_sentences_en_0, 20)\n",
    "X_test_padded_en_18  = pad_vectorized_sentences(vectorized_test_sentences_en, 20)\n",
    "#EN Fold 0\n",
    "experiment = \"En fold 1 more padding\"\n",
    "n_epochs = 8\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= torch.tensor(X_train_padded_en_0_18.astype('float32'))\n",
    "Y= Y_train_en_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = torch.tensor(X_test_padded_en_18.astype('float32'))\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dafe00",
   "metadata": {},
   "source": [
    "### Fold 0 base with different optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c3db1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 14s (1 12.5%) 0.6200]\n",
      "[0m 28s (2 25.0%) 0.4661]\n",
      "[0m 42s (3 37.5%) 0.1263]\n",
      "[0m 56s (4 50.0%) 0.1010]\n",
      "[1m 10s (5 62.5%) 0.0960]\n",
      "[1m 23s (6 75.0%) 0.0804]\n",
      "[1m 37s (7 87.5%) 0.0997]\n",
      "[1m 51s (8 100.0%) 0.0991]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.685782967032967\n",
      "Precision: 0.8176240936977134\n",
      "Recall: 0.42784182110024804\n",
      "F1: 0.561739630232781\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "experiment = \"en fold 0 adam\"\n",
    "n_epochs = 8\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.01\n",
    "optimizer = 'Adam'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_0\n",
    "Y= Y_train_en_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n optimizer: \"+ optimizer\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a65ae",
   "metadata": {},
   "source": [
    "## Try out random subsets per epoch: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f849b595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up batches\n",
    "# importing the required libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import seaborn as sns\n",
    "from torch.utils.data import TensorDataset\n",
    "import random\n",
    "\n",
    "dataset = TensorDataset(X_train_en_0, Y_train_en_0)\n",
    "  \n",
    "# implementing dataloader on the dataset \n",
    "# and printing per batch\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=200, \n",
    "                        shuffle=False)\n",
    "\n",
    "batches = []\n",
    "for batch in dataloader:\n",
    "    batches.append(batch)\n",
    "    \n",
    "\n",
    "length = len(batches)\n",
    "random_int = random.randint(0, length)\n",
    "batch_size = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "18672037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 1s (1 1.0%) 0.8927]\n",
      "[0m 2s (2 2.0%) 0.6453]\n",
      "[0m 3s (3 3.0%) 0.8848]\n",
      "[0m 4s (4 4.0%) 0.6496]\n",
      "[0m 6s (5 5.0%) 0.6290]\n",
      "[0m 7s (6 6.0%) 0.7954]\n",
      "[0m 8s (7 7.000000000000001%) 0.7954]\n",
      "[0m 9s (8 8.0%) 0.6947]\n",
      "[0m 10s (9 9.0%) 0.6541]\n",
      "[0m 12s (10 10.0%) 0.7955]\n",
      "[0m 13s (11 11.0%) 0.6347]\n",
      "[0m 14s (12 12.0%) 0.4776]\n",
      "[0m 15s (13 13.0%) 0.8781]\n",
      "[0m 17s (14 14.000000000000002%) 0.6915]\n",
      "[0m 18s (15 15.0%) 0.8757]\n",
      "[0m 19s (16 16.0%) 0.8767]\n",
      "[0m 20s (17 17.0%) 0.5224]\n",
      "[0m 21s (18 18.0%) 0.6566]\n",
      "[0m 23s (19 19.0%) 0.5225]\n",
      "[0m 24s (20 20.0%) 0.5210]\n",
      "[0m 25s (21 21.0%) 0.4794]\n",
      "[0m 26s (22 22.0%) 0.6943]\n",
      "[0m 28s (23 23.0%) 0.5218]\n",
      "[0m 29s (24 24.0%) 0.4793]\n",
      "[0m 30s (25 25.0%) 0.8311]\n",
      "[0m 31s (26 26.0%) 0.5208]\n",
      "[0m 33s (27 27.0%) 0.8734]\n",
      "[0m 34s (28 28.000000000000004%) 0.7970]\n",
      "[0m 35s (29 28.999999999999996%) 0.8283]\n",
      "[0m 36s (30 30.0%) 0.7970]\n",
      "[0m 37s (31 31.0%) 0.7969]\n",
      "[0m 39s (32 32.0%) 0.6931]\n",
      "[0m 40s (33 33.0%) 0.4762]\n",
      "[0m 41s (34 34.0%) 0.6512]\n",
      "[0m 42s (35 35.0%) 0.8737]\n",
      "[0m 44s (36 36.0%) 0.8230]\n",
      "[0m 45s (37 37.0%) 0.7990]\n",
      "[0m 46s (38 38.0%) 0.6217]\n",
      "[0m 47s (39 39.0%) 0.6483]\n",
      "[0m 48s (40 40.0%) 0.6994]\n",
      "[0m 50s (41 41.0%) 0.6092]\n",
      "[0m 51s (42 42.0%) 0.7134]\n",
      "[0m 52s (43 43.0%) 0.7266]\n",
      "[0m 53s (44 44.0%) 0.6267]\n",
      "[0m 54s (45 45.0%) 0.4265]\n",
      "[0m 56s (46 46.0%) 0.8559]\n",
      "[0m 57s (47 47.0%) 0.2131]\n",
      "[0m 58s (48 48.0%) 0.2702]\n",
      "[0m 59s (49 49.0%) 0.8442]\n",
      "[1m 1s (50 50.0%) 0.4762]\n",
      "[1m 2s (51 51.0%) 0.6164]\n",
      "[1m 3s (52 52.0%) 0.2980]\n",
      "[1m 4s (53 53.0%) 0.1646]\n",
      "[1m 6s (54 54.0%) 0.2458]\n",
      "[1m 7s (55 55.00000000000001%) 0.2123]\n",
      "[1m 8s (56 56.00000000000001%) 0.1424]\n",
      "[1m 9s (57 56.99999999999999%) 0.2125]\n",
      "[1m 10s (58 57.99999999999999%) 0.0929]\n",
      "[1m 12s (59 59.0%) 0.2187]\n",
      "[1m 13s (60 60.0%) 0.1574]\n",
      "[1m 14s (61 61.0%) 0.2797]\n",
      "[1m 15s (62 62.0%) 0.1204]\n",
      "[1m 16s (63 63.0%) 0.1013]\n",
      "[1m 18s (64 64.0%) 0.8627]\n",
      "[1m 19s (65 65.0%) 0.0449]\n",
      "[1m 20s (66 66.0%) 0.0551]\n",
      "[1m 21s (67 67.0%) 0.0546]\n",
      "[1m 22s (68 68.0%) 0.0911]\n",
      "[1m 24s (69 69.0%) 0.1951]\n",
      "[1m 25s (70 70.0%) 0.1360]\n",
      "[1m 26s (71 71.0%) 0.1026]\n",
      "[1m 27s (72 72.0%) 0.1328]\n",
      "[1m 29s (73 73.0%) 0.0444]\n",
      "[1m 30s (74 74.0%) 0.1397]\n",
      "[1m 31s (75 75.0%) 0.0814]\n",
      "[1m 32s (76 76.0%) 0.0946]\n",
      "[1m 33s (77 77.0%) 1.2486]\n",
      "[1m 35s (78 78.0%) 0.1736]\n",
      "[1m 36s (79 79.0%) 0.0910]\n",
      "[1m 37s (80 80.0%) 0.0819]\n",
      "[1m 38s (81 81.0%) 0.0446]\n",
      "[1m 39s (82 82.0%) 0.1337]\n",
      "[1m 41s (83 83.0%) 0.0397]\n",
      "[1m 42s (84 84.0%) 0.0637]\n",
      "[1m 43s (85 85.0%) 0.0660]\n",
      "[1m 44s (86 86.0%) 0.1276]\n",
      "[1m 45s (87 87.0%) 0.0676]\n",
      "[1m 47s (88 88.0%) 1.3536]\n",
      "[1m 48s (89 89.0%) 0.1209]\n",
      "[1m 49s (90 90.0%) 0.0604]\n",
      "[1m 50s (91 91.0%) 0.1291]\n",
      "[1m 51s (92 92.0%) 0.7432]\n",
      "[1m 53s (93 93.0%) 0.1042]\n",
      "[1m 54s (94 94.0%) 0.1202]\n",
      "[1m 55s (95 95.0%) 1.1388]\n",
      "[1m 56s (96 96.0%) 0.2253]\n",
      "[1m 57s (97 97.0%) 1.3474]\n",
      "[1m 59s (98 98.0%) 0.0414]\n",
      "[2m 0s (99 99.0%) 0.1127]\n",
      "[2m 1s (100 100.0%) 0.0607]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.792032967032967\n",
      "Precision: 0.8025628856193641\n",
      "Recall: 0.7402597402597403\n",
      "F1: 0.7701533323212388\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#EN Fold 0 WITH random batches per epoch (individual examples are still fed to the network, they are just taken from random \n",
    "#limited in size, subsets over multiple epochs)\n",
    "experiment = \"en_batches_0\"\n",
    "n_epochs = 100\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer_key = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_0\n",
    "Y= Y_train_en_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "\n",
    "input_size = X.size()[2] \n",
    "seq_len    = X.size()[1] \n",
    "\n",
    "model = LSTM(input_size, hidden_size, n_layers)\n",
    "\n",
    "#criterion = loss_function\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "optimizer = 0\n",
    "\n",
    "if optimizer_key == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    random_int = random.randint(0, length-2)\n",
    "    for i in range(batch_size): \n",
    "        \n",
    "        loss = train(model,optimizer, batches[random_int][0][i].view(1, 15,100 ),batches[random_int][1][i])\n",
    "        loss_avg += loss\n",
    "    print('[{} ({} {}%) {:.4f}]'.format(time_since(start), epoch, epoch/n_epochs * 100, loss))\n",
    "\n",
    "\n",
    "all_losses.append(loss_avg/ n_epochs)\n",
    "loss_avg = 0\n",
    "\n",
    "\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626f38bf",
   "metadata": {},
   "source": [
    "### Whole dataset (folds merged):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6e09c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 36s (1 12.5%) 0.7861]\n",
      "[1m 11s (2 25.0%) 0.2022]\n",
      "[1m 46s (3 37.5%) 0.2366]\n",
      "[2m 21s (4 50.0%) 0.2309]\n",
      "[2m 58s (5 62.5%) 0.2418]\n",
      "[3m 33s (6 75.0%) 0.2354]\n",
      "[4m 9s (7 87.5%) 0.2345]\n",
      "[4m 44s (8 100.0%) 0.2396]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.7820741758241758\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "experiment = \"merged en\"\n",
    "n_epochs = 8\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en\n",
    "Y= Y_train_en\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8cae97",
   "metadata": {},
   "source": [
    "\n",
    "## FRENCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfecfa5",
   "metadata": {},
   "source": [
    "### Fold 0, base configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ec13ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 13s (1 12.5%) 0.8462]\n",
      "[0m 27s (2 25.0%) 0.8421]\n",
      "[0m 40s (3 37.5%) 0.8415]\n",
      "[0m 54s (4 50.0%) 0.8414]\n",
      "[1m 8s (5 62.5%) 0.8415]\n",
      "[1m 22s (6 75.0%) 0.8416]\n",
      "[1m 36s (7 87.5%) 0.8417]\n",
      "[1m 50s (8 100.0%) 0.8420]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-52d351434a4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precision: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-bbd72bb94840>\u001b[0m in \u001b[0;36mmetrics\u001b[0;34m(model, X_test, Y_test)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "#FR Fold 0\n",
    "experiment = \"FR 0 base\"\n",
    "n_epochs = 8\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_fr_0\n",
    "Y= Y_train_fr_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_fr\n",
    "Y_test = Y_test_fr\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef9d63c",
   "metadata": {},
   "source": [
    "###  Fold 0,  new base config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab3f82b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 13s (1 10.0%) 0.9207]\n",
      "[0m 27s (2 20.0%) 0.9200]\n",
      "[0m 41s (3 30.0%) 0.9201]\n",
      "[0m 55s (4 40.0%) 0.9202]\n",
      "[1m 9s (5 50.0%) 0.9204]\n",
      "[1m 23s (6 60.0%) 0.9207]\n",
      "[1m 37s (7 70.0%) 0.9214]\n",
      "[1m 51s (8 80.0%) 0.9232]\n",
      "[2m 4s (9 90.0%) 0.9376]\n",
      "[2m 18s (10 100.0%) 0.4702]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.5867445054945055\n",
      "Precision: 0.5815450643776824\n",
      "Recall: 0.434991974317817\n",
      "F1: 0.49770431588613406\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "experiment = \"FR 0 new base config\"\n",
    "n_epochs = 10\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.15\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_fr_0\n",
    "Y= Y_train_fr_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_fr\n",
    "Y_test = Y_test_fr\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0d5b1c",
   "metadata": {},
   "source": [
    "###  Fold 1, base config \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd02608f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 13s (1 10.0%) 0.8300]\n",
      "[0m 27s (2 20.0%) 0.8301]\n",
      "[0m 41s (3 30.0%) 0.8301]\n",
      "[0m 54s (4 40.0%) 0.8301]\n",
      "[1m 8s (5 50.0%) 0.8301]\n",
      "[1m 21s (6 60.0%) 0.8300]\n",
      "[1m 35s (7 70.0%) 0.8300]\n",
      "[1m 49s (8 80.0%) 0.8299]\n",
      "[2m 2s (9 90.0%) 0.8297]\n",
      "[2m 16s (10 100.0%) 0.8295]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.4706730769230769\n",
      "Precision: 0.4706730769230769\n",
      "Recall: 1.0\n",
      "F1: 0.6400784570120954\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "experiment = \"FR 1 new base config\"\n",
    "n_epochs = 10\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.15\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_fr_1\n",
    "Y= Y_train_fr_1\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_fr\n",
    "Y_test = Y_test_fr\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7404102",
   "metadata": {},
   "source": [
    "###  Fold 2, base config  + n epoch = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18e690bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 13s (1 10.0%) 0.4466]\n",
      "[0m 27s (2 20.0%) 0.4469]\n",
      "[0m 41s (3 30.0%) 0.4468]\n",
      "[0m 55s (4 40.0%) 0.4467]\n",
      "[1m 9s (5 50.0%) 0.4466]\n",
      "[1m 23s (6 60.0%) 0.4463]\n",
      "[1m 37s (7 70.0%) 0.4458]\n",
      "[1m 51s (8 80.0%) 0.4447]\n",
      "[2m 5s (9 90.0%) 0.4398]\n",
      "[2m 19s (10 100.0%) 0.2703]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.4706730769230769\n",
      "Precision: 0.4706730769230769\n",
      "Recall: 1.0\n",
      "F1: 0.6400784570120954\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#FR Fold 0\n",
    "experiment = \"FR base config 10 ep\"\n",
    "n_epochs = 10\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.15\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_fr_2\n",
    "Y= Y_train_fr_2\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_fr\n",
    "Y_test = Y_test_fr\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5423e056",
   "metadata": {},
   "source": [
    "###  Fold 0, base config  + n epoch = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aca124f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 14s (1 6.666666666666667%) 0.9206]\n",
      "[0m 28s (2 13.333333333333334%) 0.9199]\n",
      "[0m 42s (3 20.0%) 0.9198]\n",
      "[0m 56s (4 26.666666666666668%) 0.9199]\n",
      "[1m 10s (5 33.33333333333333%) 0.9199]\n",
      "[1m 24s (6 40.0%) 0.9199]\n",
      "[1m 38s (7 46.666666666666664%) 0.9200]\n",
      "[1m 52s (8 53.333333333333336%) 0.9201]\n",
      "[2m 6s (9 60.0%) 0.9202]\n",
      "[2m 20s (10 66.66666666666666%) 0.9204]\n",
      "[2m 34s (11 73.33333333333333%) 0.9207]\n",
      "[2m 48s (12 80.0%) 0.9213]\n",
      "[3m 2s (13 86.66666666666667%) 0.9232]\n",
      "[3m 16s (14 93.33333333333333%) 0.9379]\n",
      "[3m 30s (15 100.0%) 0.4694]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.5163461538461539\n",
      "Precision: 0.48636166835041134\n",
      "Recall: 0.4917554355756603\n",
      "F1: 0.48904368016253086\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "experiment = \"FR base config nepch 15\"\n",
    "n_epochs = 15\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.15\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_fr_0\n",
    "Y= Y_train_fr_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_fr\n",
    "Y_test = Y_test_fr\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "428ca1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 13s (1 4.0%) 0.9205]\n",
      "[0m 28s (2 8.0%) 0.9199]\n",
      "[0m 42s (3 12.0%) 0.9199]\n",
      "[0m 57s (4 16.0%) 0.9199]\n",
      "[1m 12s (5 20.0%) 0.9200]\n",
      "[1m 26s (6 24.0%) 0.9201]\n",
      "[1m 40s (7 28.000000000000004%) 0.9202]\n",
      "[1m 55s (8 32.0%) 0.9205]\n",
      "[2m 8s (9 36.0%) 0.9209]\n",
      "[2m 22s (10 40.0%) 0.9220]\n",
      "[2m 36s (11 44.0%) 0.9266]\n",
      "[2m 50s (12 48.0%) 1.2812]\n",
      "[3m 4s (13 52.0%) 0.0881]\n",
      "[3m 17s (14 56.00000000000001%) 0.2227]\n",
      "[3m 31s (15 60.0%) 0.1400]\n",
      "[3m 45s (16 64.0%) 0.1976]\n",
      "[3m 59s (17 68.0%) 0.3274]\n",
      "[4m 13s (18 72.0%) 0.2263]\n",
      "[4m 26s (19 76.0%) 0.1979]\n",
      "[4m 41s (20 80.0%) 1.1215]\n",
      "[4m 55s (21 84.0%) 1.9567]\n",
      "[5m 8s (22 88.0%) 0.2067]\n",
      "[5m 22s (23 92.0%) 0.1830]\n",
      "[5m 36s (24 96.0%) 0.1758]\n",
      "[5m 50s (25 100.0%) 0.1868]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.8074175824175824\n",
      "Precision: 0.7829489867225716\n",
      "Recall: 0.8174522107106377\n",
      "F1: 0.7998286693318104\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "experiment = \"FR more epochs\"\n",
    "n_epochs = 25\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.15\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_fr_0\n",
    "Y= Y_train_fr_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_fr\n",
    "Y_test = Y_test_fr\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8edcece8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 13s (1 2.857142857142857%) 0.9205]\n",
      "[0m 27s (2 5.714285714285714%) 0.9199]\n",
      "[0m 41s (3 8.571428571428571%) 0.9199]\n",
      "[0m 54s (4 11.428571428571429%) 0.9199]\n",
      "[1m 8s (5 14.285714285714285%) 0.9200]\n",
      "[1m 23s (6 17.142857142857142%) 0.9200]\n",
      "[1m 37s (7 20.0%) 0.9201]\n",
      "[1m 52s (8 22.857142857142858%) 0.9203]\n",
      "[2m 6s (9 25.71428571428571%) 0.9205]\n",
      "[2m 21s (10 28.57142857142857%) 0.9210]\n",
      "[2m 35s (11 31.428571428571427%) 0.9223]\n",
      "[2m 50s (12 34.285714285714285%) 0.9283]\n",
      "[3m 5s (13 37.142857142857146%) 1.6254]\n",
      "[3m 20s (14 40.0%) 0.3962]\n",
      "[3m 34s (15 42.857142857142854%) 0.2122]\n",
      "[3m 49s (16 45.714285714285715%) 0.1830]\n",
      "[4m 3s (17 48.57142857142857%) 0.1704]\n",
      "[4m 18s (18 51.42857142857142%) 0.1739]\n",
      "[4m 32s (19 54.285714285714285%) 0.1635]\n",
      "[4m 47s (20 57.14285714285714%) 0.1767]\n",
      "[5m 1s (21 60.0%) 0.1805]\n",
      "[5m 16s (22 62.857142857142854%) 0.1876]\n",
      "[5m 30s (23 65.71428571428571%) 0.2129]\n",
      "[5m 45s (24 68.57142857142857%) 0.2067]\n",
      "[6m 0s (25 71.42857142857143%) 0.1961]\n",
      "[6m 15s (26 74.28571428571429%) 0.1716]\n",
      "[6m 30s (27 77.14285714285715%) 0.1667]\n",
      "[6m 45s (28 80.0%) 0.1881]\n",
      "[7m 0s (29 82.85714285714286%) 0.1771]\n",
      "[7m 15s (30 85.71428571428571%) 0.1838]\n",
      "[7m 30s (31 88.57142857142857%) 0.1742]\n",
      "[7m 45s (32 91.42857142857143%) 0.1599]\n",
      "[8m 0s (33 94.28571428571428%) 1.1205]\n",
      "[8m 16s (34 97.14285714285714%) 0.1836]\n",
      "[8m 31s (35 100.0%) 0.1911]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.7757554945054945\n",
      "Precision: 0.7578327105490084\n",
      "Recall: 0.7694440391069605\n",
      "F1: 0.7635942364781696\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "experiment = \"FR more epochs\"\n",
    "n_epochs = 35\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.15\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_fr_0\n",
    "Y= Y_train_fr_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_fr\n",
    "Y_test = Y_test_fr\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2ff056",
   "metadata": {},
   "source": [
    "### Whole dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2fd0e410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 44s (1 10.0%) 0.4470]\n",
      "[1m 28s (2 20.0%) 0.4470]\n",
      "[2m 12s (3 30.0%) 0.4468]\n",
      "[2m 58s (4 40.0%) 0.4460]\n",
      "[3m 41s (5 50.0%) 0.2411]\n",
      "[4m 26s (6 60.0%) 0.1004]\n",
      "[5m 11s (7 70.0%) 0.0874]\n",
      "[5m 56s (8 80.0%) 0.1145]\n",
      "[6m 43s (9 90.0%) 0.1314]\n",
      "[7m 29s (10 100.0%) 0.1114]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.7883928571428571\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "experiment = \"French whole dataset\"\n",
    "n_epochs = 10\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.15\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_fr\n",
    "Y= Y_train_fr\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_fr\n",
    "Y_test = Y_test_fr\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec77f2d",
   "metadata": {},
   "source": [
    "###  ITALIAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dfe6b5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 13s (1 12.5%) 0.6172]\n",
      "[0m 27s (2 25.0%) 0.6202]\n",
      "[0m 41s (3 37.5%) 0.6206]\n",
      "[0m 55s (4 50.0%) 0.6207]\n",
      "[1m 9s (5 62.5%) 0.6207]\n",
      "[1m 24s (6 75.0%) 0.6207]\n",
      "[1m 38s (7 87.5%) 0.6206]\n",
      "[1m 52s (8 100.0%) 0.6206]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.4706730769230769\n",
      "Precision: 0.4706730769230769\n",
      "Recall: 1.0\n",
      "F1: 0.6400784570120954\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "experiment = \"It 0 base\"\n",
    "n_epochs = 8\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_it_0\n",
    "Y= Y_train_it_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_it\n",
    "Y_test = Y_test_it\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cec6c900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 14s (1 10.0%) 0.9210]\n",
      "[0m 29s (2 20.0%) 0.9202]\n",
      "[0m 44s (3 30.0%) 0.9202]\n",
      "[0m 58s (4 40.0%) 0.9203]\n",
      "[1m 13s (5 50.0%) 0.9203]\n",
      "[1m 28s (6 60.0%) 0.9204]\n",
      "[1m 42s (7 70.0%) 0.9204]\n",
      "[1m 57s (8 80.0%) 0.9205]\n",
      "[2m 12s (9 90.0%) 0.9207]\n",
      "[2m 27s (10 100.0%) 0.9208]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8965ade6ecdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precision: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-bbd72bb94840>\u001b[0m in \u001b[0;36mmetrics\u001b[0;34m(model, X_test, Y_test)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "experiment = \"It base 1 \"\n",
    "n_epochs = 10\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.15\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_it_1\n",
    "Y= Y_train_it_1\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_it\n",
    "Y_test = Y_test_it\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2375a907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 14s (1 6.666666666666667%) 0.6118]\n",
      "[0m 29s (2 13.333333333333334%) 0.6119]\n",
      "[0m 43s (3 20.0%) 0.6119]\n",
      "[0m 58s (4 26.666666666666668%) 0.6119]\n",
      "[1m 13s (5 33.33333333333333%) 0.6118]\n",
      "[1m 28s (6 40.0%) 0.6118]\n",
      "[1m 43s (7 46.666666666666664%) 0.6118]\n",
      "[1m 57s (8 53.333333333333336%) 0.6118]\n",
      "[2m 12s (9 60.0%) 0.6118]\n",
      "[2m 26s (10 66.66666666666666%) 0.6118]\n",
      "[2m 41s (11 73.33333333333333%) 0.6117]\n",
      "[2m 55s (12 80.0%) 0.6116]\n",
      "[3m 10s (13 86.66666666666667%) 0.6115]\n",
      "[3m 26s (14 93.33333333333333%) 0.6114]\n",
      "[3m 41s (15 100.0%) 0.6110]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.4706730769230769\n",
      "Precision: 0.4706730769230769\n",
      "Recall: 1.0\n",
      "F1: 0.6400784570120954\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "experiment = \"It base 2\"\n",
    "n_epochs = 15\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.15\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_it_2\n",
    "Y= Y_train_it_2\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_it\n",
    "Y_test = Y_test_it\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc2dd5d",
   "metadata": {},
   "source": [
    "### base, more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db7fc05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 14s (1 6.666666666666667%) 0.6172]\n",
      "[0m 29s (2 13.333333333333334%) 0.6202]\n",
      "[0m 44s (3 20.0%) 0.6207]\n",
      "[0m 58s (4 26.666666666666668%) 0.6208]\n",
      "[1m 13s (5 33.33333333333333%) 0.6208]\n",
      "[1m 28s (6 40.0%) 0.6208]\n",
      "[1m 42s (7 46.666666666666664%) 0.6208]\n",
      "[1m 58s (8 53.333333333333336%) 0.6207]\n",
      "[2m 12s (9 60.0%) 0.6207]\n",
      "[2m 27s (10 66.66666666666666%) 0.6207]\n",
      "[2m 41s (11 73.33333333333333%) 0.6206]\n",
      "[2m 56s (12 80.0%) 0.6206]\n",
      "[3m 11s (13 86.66666666666667%) 0.6205]\n",
      "[3m 25s (14 93.33333333333333%) 0.6205]\n",
      "[3m 40s (15 100.0%) 0.6204]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.4706730769230769\n",
      "Precision: 0.4706730769230769\n",
      "Recall: 1.0\n",
      "F1: 0.6400784570120954\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "experiment = \"It base more epochs (15)\"\n",
    "n_epochs = 25\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_it_0\n",
    "Y= Y_train_it_0\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_it\n",
    "Y_test = Y_test_it\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03eb43f",
   "metadata": {},
   "source": [
    "### Whole dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3dfb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"It whole dataset\"\n",
    "n_epochs = 8\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.15\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_it\n",
    "Y= Y_train_it\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_it\n",
    "Y_test = Y_test_it\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a75664",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "789b8614",
   "metadata": {},
   "source": [
    "### Extra:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab4b90d",
   "metadata": {},
   "source": [
    "#### EN fold 0 with Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "56e084e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 12s (1 12.5%) 0.5793]\n",
      "[0m 25s (2 25.0%) 0.1364]\n",
      "[0m 37s (3 37.5%) 0.0731]\n",
      "[0m 51s (4 50.0%) 0.0745]\n",
      "[1m 3s (5 62.5%) 0.0825]\n",
      "[1m 16s (6 75.0%) 0.0771]\n",
      "[1m 29s (7 87.5%) 0.0876]\n",
      "[1m 42s (8 100.0%) 0.0802]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.5548763736263737\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "experiment = 2\n",
    "n_epochs = 8\n",
    "hidden_size = 256\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'Adagrad'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_0\n",
    "Y= Y_train_en_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n optimizer: \"+ optimizer\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
