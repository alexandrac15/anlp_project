{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31adb9b5",
   "metadata": {},
   "source": [
    "# Reading the data.\n",
    "The data is given in 3 languages: English, French  and Italian.We are provided with 20,394 artificially  generated sentences with 30 semantic categories  for each of the languages, of which 5,838 sentences  are training samples and the other 14,556 samples  are test samples. The French and Italian are slightly adapted translations of the English dataset. \n",
    "\n",
    "\n",
    "In the baseline every fold is calculated separately and we decided to see the result with merging the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c82990fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Read the csv file\n",
    "#Task 1: concatinate training data  and uplead test data for English,French and Italian datasets.\n",
    "#1.1 English.\n",
    "data1_train_en= pd.read_table('En-Subtask1-fold_0.tsv',sep='\\t')\n",
    "data2_train_en= pd.read_table('En-Subtask1-fold_1.tsv',sep='\\t')\n",
    "data3_train_en= pd.read_table('En-Subtask1-fold_2.tsv',sep='\\t')\n",
    "train_concat_en=pd.concat([data1_train_en,data2_train_en,data3_train_en],join='outer',ignore_index=True)\n",
    "train_concat_en.drop(columns=['ID'],inplace=True)\n",
    "#train_concat_en.to_csv(r'Training_English.csv', header=True,index=False)\n",
    "test_en= pd.read_csv('En-Subtask1-labels.tsv',sep='\\t')\n",
    "#1.2 French.\n",
    "data1_train_fr= pd.read_table('Fr-Subtask1-fold_0.tsv',sep='\\t')\n",
    "data2_train_fr= pd.read_table('Fr-Subtask1-fold_1.tsv',sep='\\t')\n",
    "data3_train_fr= pd.read_table('Fr-Subtask1-fold_2.tsv',sep='\\t')\n",
    "train_concat_fr=pd.concat([data1_train_fr,data2_train_fr,data3_train_fr],join='outer',ignore_index=True)\n",
    "train_concat_fr.drop(columns=['ID'],inplace=True)\n",
    "#train_concat_fr.to_csv(r'Training_French.csv', header=True,index=False)\n",
    "test_fr= pd.read_csv('Fr-Subtask1-labels.tsv',sep='\\t')\n",
    "#1.3 Italian.\n",
    "data1_train_it= pd.read_table('It-Subtask1-fold_0.tsv',sep='\\t')\n",
    "data2_train_it= pd.read_table('It-Subtask1-fold_1.tsv',sep='\\t')\n",
    "data3_train_it= pd.read_table('It-Subtask1-fold_2.tsv',sep='\\t')\n",
    "train_concat_it=pd.concat([data1_train_it,data2_train_it,data3_train_it],join='outer',ignore_index=True)\n",
    "train_concat_it.drop(columns=['ID'],inplace=True)\n",
    "#train_concat_it.to_csv(r'Training_Italian.csv', header=True,index=False)\n",
    "test_it= pd.read_csv('It-Subtask1-labels.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f32ad295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_concat_en)==len(train_concat_fr)==len(train_concat_it)\n",
    "#Merged data is the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a24b32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_en)==len(test_fr)==len(test_it)\n",
    "#len(test_en)=14560"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730711af",
   "metadata": {},
   "source": [
    "# Baseline.\n",
    "The code below was taken from the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bbad70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "\"English data.\"\n",
    "train_texts_en=(train_concat_en['Sentence'])\n",
    "y_train_en=np.array(train_concat_en['Labels'])\n",
    "dev_texts_en=(test_en['Sentence'])\n",
    "test_true_y_en=np.array(test_en['Labels'])   \n",
    "#Vectorization_enlgish.\n",
    "tfidf_vectorizer = TfidfVectorizer(encoding='utf-8', lowercase=True, ngram_range=(1, 3),\n",
    "                                   norm='l1',use_idf=True, max_df=0.95, min_df=5, max_features=1000) \n",
    "train_x_feat_en= tfidf_vectorizer.fit_transform(train_texts_en)\n",
    "classifier_svc_en= LinearSVC(random_state=42, C=1.0, class_weight='balanced', tol=0.0001).fit(train_x_feat_en,                                             \n",
    "                                                                                              y_train_en)\n",
    "##Testing_english:\n",
    "test_x_feat_en= tfidf_vectorizer.transform(dev_texts_en)\n",
    "test_pred_y_en= classifier_svc_en.predict(test_x_feat_en)\n",
    "\n",
    "\"French data.\"\n",
    "train_texts_fr=(train_concat_fr['Sentence'])\n",
    "y_train_fr=np.array(train_concat_fr['Labels'])\n",
    "dev_texts_fr=(test_en['Sentence'])\n",
    "test_true_y_fr=np.array(test_fr['Labels'])   \n",
    "#Vectorization_french.\n",
    "train_x_feat_fr= tfidf_vectorizer.fit_transform(train_texts_fr)\n",
    "classifier_svc_fr= LinearSVC(random_state=42, C=1.0, class_weight='balanced', tol=0.0001).fit(train_x_feat_fr,                                             \n",
    "                                                                                              y_train_fr)\n",
    "##Testing_french:\n",
    "test_x_feat_fr= tfidf_vectorizer.transform(dev_texts_fr)\n",
    "test_pred_y_fr= classifier_svc_en.predict(test_x_feat_fr)\n",
    "\n",
    "\"Italian data.\"\n",
    "\n",
    "train_texts_it=(train_concat_it['Sentence'])\n",
    "y_train_it=np.array(train_concat_it['Labels'])\n",
    "dev_texts_it=(test_it['Sentence'])\n",
    "test_true_y_it=np.array(test_it['Labels'])   \n",
    "#Vectorization_french.\n",
    "train_x_feat_it= tfidf_vectorizer.fit_transform(train_texts_it)\n",
    "classifier_svc_it= LinearSVC(random_state=42, C=1.0, class_weight='balanced', tol=0.0001).fit(train_x_feat_it,                                             \n",
    "                                                                                              y_train_it)\n",
    "##Testing_french:\n",
    "test_x_feat_it= tfidf_vectorizer.transform(dev_texts_it)\n",
    "test_pred_y_it= classifier_svc_en.predict(test_x_feat_it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4546705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Results for English data ==========\n",
      "Precision                                   : 0.642\n",
      "Recall                                      : 0.857\n",
      "F1-Score                                    : 0.734\n",
      "========== Results for French data ==========\n",
      "Precision                                   : 0.470\n",
      "Recall                                      : 0.890\n",
      "F1-Score                                    : 0.616\n",
      "========== Results for Italian data ==========\n",
      "Precision                                   : 0.488\n",
      "Recall                                      : 0.619\n",
      "F1-Score                                    : 0.546\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "print(\"=\"*10+\" Results for English data \"+\"=\"*10)\n",
    "print('Precision                                   : %.3f'%precision_score(test_true_y_en, test_pred_y_en))\n",
    "print('Recall                                      : %.3f'%recall_score(test_true_y_en, test_pred_y_en))\n",
    "print('F1-Score                                    : %.3f'%f1_score(test_true_y_en, test_pred_y_en))\n",
    "print(\"=\"*10+\" Results for French data \"+\"=\"*10)\n",
    "print('Precision                                   : %.3f'%precision_score(test_true_y_fr, test_pred_y_fr))\n",
    "print('Recall                                      : %.3f'%recall_score(test_true_y_fr, test_pred_y_fr))\n",
    "print('F1-Score                                    : %.3f'%f1_score(test_true_y_fr, test_pred_y_fr))\n",
    "print(\"=\"*10+\" Results for Italian data \"+\"=\"*10)\n",
    "print('Precision                                   : %.3f'%precision_score(test_true_y_it, test_pred_y_it))\n",
    "print('Recall                                      : %.3f'%recall_score(test_true_y_it, test_pred_y_it))\n",
    "print('F1-Score                                    : %.3f'%f1_score(test_true_y_it, test_pred_y_it))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162e06ae",
   "metadata": {},
   "source": [
    " As observed,the baseline was uploaded on the 31st of October 2021 and labled  test dataset was added 3 month after on the 5th of February 2022.Moreover,the length of traning dataset is 5837 and the length of test dataset is 14560.This factors could affect the evaluation of the model,even though it was same as it was baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c246f50",
   "metadata": {},
   "source": [
    "# Preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc5671df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import spacy\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import snowballstemmer\n",
    "\n",
    "stemmer = snowballstemmer.stemmer('english');\n",
    "import re\n",
    "def remove_punct(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = \" \".join(re.split('\\W+', text.lower()))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "def lemmatizing(text):\n",
    "    text_l= \"\".join([wn.lemmatize(word) for word in text])\n",
    "    return text_l\n",
    "\n",
    "#nlp_en=spacy.load(\"en\")\n",
    "\n",
    "#def stemming_spacy_en(text):\n",
    "   # text_load=[]\n",
    "    #for line in text:\n",
    "      #  text_load.extend(nlp_en(line))\n",
    "     #   for word in text_load:\n",
    "            \n",
    "     \n",
    "    return text_lem\n",
    "\n",
    "snow_stemmer_en= SnowballStemmer(language='english')\n",
    "snow_stemmer_fr= SnowballStemmer(language='french')\n",
    "snow_stemmer_it= SnowballStemmer(language='italian')\n",
    "\n",
    "def stemming_en(text):\n",
    "    text_s= \"\".join([snow_stemmer_en.stem(word) for word in text])\n",
    "    return text_s\n",
    "\n",
    "def stemming_fr(text):\n",
    "    text_s= \"\".join([snow_stemmer_fr.stem(word) for word in text])\n",
    "    return text_s\n",
    "\n",
    "\n",
    "def stemming_it(text):\n",
    "    text_s= \"\".join([snow_stemmer_fr.stem(word) for word in text])\n",
    "    return text_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87bbf75",
   "metadata": {},
   "source": [
    "English data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61d0b19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Labels</th>\n",
       "      <th>Sentence_no_punct</th>\n",
       "      <th>Sentence_tokenized</th>\n",
       "      <th>Sentence_lemmatized</th>\n",
       "      <th>Sentence_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I like ham , but not fish .</td>\n",
       "      <td>1</td>\n",
       "      <td>I like ham  but not fish</td>\n",
       "      <td>i like ham but not fish</td>\n",
       "      <td>i like ham but not fish</td>\n",
       "      <td>i like ham but not fish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I like restaurants , and clerks too .</td>\n",
       "      <td>1</td>\n",
       "      <td>I like restaurants  and clerks too</td>\n",
       "      <td>i like restaurants and clerks too</td>\n",
       "      <td>i like restaurants and clerks too</td>\n",
       "      <td>i like restaurants and clerks too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I like jewelry more than skirts .</td>\n",
       "      <td>1</td>\n",
       "      <td>I like jewelry more than skirts</td>\n",
       "      <td>i like jewelry more than skirts</td>\n",
       "      <td>i like jewelry more than skirts</td>\n",
       "      <td>i like jewelry more than skirts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I like seafood , and veal too .</td>\n",
       "      <td>1</td>\n",
       "      <td>I like seafood  and veal too</td>\n",
       "      <td>i like seafood and veal too</td>\n",
       "      <td>i like seafood and veal too</td>\n",
       "      <td>i like seafood and veal too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I like pets , and jellyfish too .</td>\n",
       "      <td>1</td>\n",
       "      <td>I like pets  and jellyfish too</td>\n",
       "      <td>i like pets and jellyfish too</td>\n",
       "      <td>i like pets and jellyfish too</td>\n",
       "      <td>i like pets and jellyfish too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5832</th>\n",
       "      <td>I like Chianti , and more specifically beer .</td>\n",
       "      <td>0</td>\n",
       "      <td>I like Chianti  and more specifically beer</td>\n",
       "      <td>i like chianti and more specifically beer</td>\n",
       "      <td>i like chianti and more specifically beer</td>\n",
       "      <td>i like chianti and more specifically beer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5833</th>\n",
       "      <td>I like necklaces , and more specifically glass...</td>\n",
       "      <td>0</td>\n",
       "      <td>I like necklaces  and more specifically glasses</td>\n",
       "      <td>i like necklaces and more specifically glasses</td>\n",
       "      <td>i like necklaces and more specifically glasses</td>\n",
       "      <td>i like necklaces and more specifically glasses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5834</th>\n",
       "      <td>He trusts his senses , except his hearing .</td>\n",
       "      <td>1</td>\n",
       "      <td>He trusts his senses  except his hearing</td>\n",
       "      <td>he trusts his senses except his hearing</td>\n",
       "      <td>he trusts his senses except his hearing</td>\n",
       "      <td>he trusts his senses except his hearing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5835</th>\n",
       "      <td>I like Merlot , and more specifically sprite .</td>\n",
       "      <td>0</td>\n",
       "      <td>I like Merlot  and more specifically sprite</td>\n",
       "      <td>i like merlot and more specifically sprite</td>\n",
       "      <td>i like merlot and more specifically sprite</td>\n",
       "      <td>i like merlot and more specifically sprite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5836</th>\n",
       "      <td>I do not like skirts , I prefer pets .</td>\n",
       "      <td>1</td>\n",
       "      <td>I do not like skirts  I prefer pets</td>\n",
       "      <td>i do not like skirts i prefer pets</td>\n",
       "      <td>i do not like skirts i prefer pets</td>\n",
       "      <td>i do not like skirts i prefer pets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5837 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence  Labels  \\\n",
       "0                           I like ham , but not fish .       1   \n",
       "1                 I like restaurants , and clerks too .       1   \n",
       "2                     I like jewelry more than skirts .       1   \n",
       "3                       I like seafood , and veal too .       1   \n",
       "4                     I like pets , and jellyfish too .       1   \n",
       "...                                                 ...     ...   \n",
       "5832      I like Chianti , and more specifically beer .       0   \n",
       "5833  I like necklaces , and more specifically glass...       0   \n",
       "5834        He trusts his senses , except his hearing .       1   \n",
       "5835     I like Merlot , and more specifically sprite .       0   \n",
       "5836             I do not like skirts , I prefer pets .       1   \n",
       "\n",
       "                                     Sentence_no_punct  \\\n",
       "0                            I like ham  but not fish    \n",
       "1                  I like restaurants  and clerks too    \n",
       "2                     I like jewelry more than skirts    \n",
       "3                        I like seafood  and veal too    \n",
       "4                      I like pets  and jellyfish too    \n",
       "...                                                ...   \n",
       "5832       I like Chianti  and more specifically beer    \n",
       "5833  I like necklaces  and more specifically glasses    \n",
       "5834         He trusts his senses  except his hearing    \n",
       "5835      I like Merlot  and more specifically sprite    \n",
       "5836              I do not like skirts  I prefer pets    \n",
       "\n",
       "                                   Sentence_tokenized  \\\n",
       "0                            i like ham but not fish    \n",
       "1                  i like restaurants and clerks too    \n",
       "2                    i like jewelry more than skirts    \n",
       "3                        i like seafood and veal too    \n",
       "4                      i like pets and jellyfish too    \n",
       "...                                               ...   \n",
       "5832       i like chianti and more specifically beer    \n",
       "5833  i like necklaces and more specifically glasses    \n",
       "5834         he trusts his senses except his hearing    \n",
       "5835      i like merlot and more specifically sprite    \n",
       "5836              i do not like skirts i prefer pets    \n",
       "\n",
       "                                  Sentence_lemmatized  \\\n",
       "0                            i like ham but not fish    \n",
       "1                  i like restaurants and clerks too    \n",
       "2                    i like jewelry more than skirts    \n",
       "3                        i like seafood and veal too    \n",
       "4                      i like pets and jellyfish too    \n",
       "...                                               ...   \n",
       "5832       i like chianti and more specifically beer    \n",
       "5833  i like necklaces and more specifically glasses    \n",
       "5834         he trusts his senses except his hearing    \n",
       "5835      i like merlot and more specifically sprite    \n",
       "5836              i do not like skirts i prefer pets    \n",
       "\n",
       "                                     Sentence_stemmed  \n",
       "0                            i like ham but not fish   \n",
       "1                  i like restaurants and clerks too   \n",
       "2                    i like jewelry more than skirts   \n",
       "3                        i like seafood and veal too   \n",
       "4                      i like pets and jellyfish too   \n",
       "...                                               ...  \n",
       "5832       i like chianti and more specifically beer   \n",
       "5833  i like necklaces and more specifically glasses   \n",
       "5834         he trusts his senses except his hearing   \n",
       "5835      i like merlot and more specifically sprite   \n",
       "5836              i do not like skirts i prefer pets   \n",
       "\n",
       "[5837 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_concat_en['Sentence_no_punct'] = train_concat_en['Sentence'].apply(lambda x: remove_punct(x))\n",
    "train_concat_en['Sentence_tokenized'] = train_concat_en['Sentence_no_punct'].apply(lambda x: tokenize(x))\n",
    "train_concat_en['Sentence_lemmatized'] = train_concat_en['Sentence_tokenized'].apply(lambda x: lemmatizing(x))\n",
    "train_concat_en['Sentence_stemmed'] = train_concat_en['Sentence_tokenized'].apply(lambda x: stemming_en(x))\n",
    "#train_concat_en['Sentence_stemmed_spacy'] = train_concat_en['Sentence_tokenized'].apply(lambda x: stemming_spacy_en(x))\n",
    "train_concat_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "273dcd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_en['Sentence_no_punct'] = test_en['Sentence'].apply(lambda x: remove_punct(x))\n",
    "test_en['Sentence_tokenized'] = test_en['Sentence_no_punct'].apply(lambda x: tokenize(x))\n",
    "test_en['Sentence_stemmed'] = test_en['Sentence_tokenized'].apply(lambda x: stemming_en(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd81db7f",
   "metadata": {},
   "source": [
    "French data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3802006e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Labels</th>\n",
       "      <th>Sentence_no_punct</th>\n",
       "      <th>Sentence_tokenized</th>\n",
       "      <th>Sentence_lemmatized</th>\n",
       "      <th>Sentence_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>J' aime la dinde , et aussi les huîtres .</td>\n",
       "      <td>1</td>\n",
       "      <td>J aime la dinde  et aussi les huîtres</td>\n",
       "      <td>j aime la dinde et aussi les huîtres</td>\n",
       "      <td>j aime la dinde et aussi les huîtres</td>\n",
       "      <td>j aime la dinde et aussi les huîtres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J' ai rencontré les astronomes , sauf les gref...</td>\n",
       "      <td>0</td>\n",
       "      <td>J ai rencontré les astronomes  sauf les greffi...</td>\n",
       "      <td>j ai rencontré les astronomes sauf les greffiers</td>\n",
       "      <td>j ai rencontré les astronomes sauf les greffiers</td>\n",
       "      <td>j ai rencontré les astronomes sauf les greffiers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J' apprécie les armoires , et plus particulièr...</td>\n",
       "      <td>0</td>\n",
       "      <td>J apprécie les armoires  et plus particulièrem...</td>\n",
       "      <td>j apprécie les armoires et plus particulièreme...</td>\n",
       "      <td>j apprécie les armoires et plus particulièreme...</td>\n",
       "      <td>j apprécie les armoires et plus particulièreme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Il n' aime pas les émotions , il préfère la lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>Il n aime pas les émotions  il préfère la logi...</td>\n",
       "      <td>il n aime pas les émotions il préfère la logique</td>\n",
       "      <td>il n aime pas les émotions il préfère la logique</td>\n",
       "      <td>il n aime pas les émotions il préfère la logique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J' aime les pantalons , sauf les chaussures .</td>\n",
       "      <td>0</td>\n",
       "      <td>J aime les pantalons  sauf les chaussures</td>\n",
       "      <td>j aime les pantalons sauf les chaussures</td>\n",
       "      <td>j aime les pantalons sauf les chaussures</td>\n",
       "      <td>j aime les pantalons sauf les chaussures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5832</th>\n",
       "      <td>J' utilise le nylon , mais pas le cuir .</td>\n",
       "      <td>1</td>\n",
       "      <td>J utilise le nylon  mais pas le cuir</td>\n",
       "      <td>j utilise le nylon mais pas le cuir</td>\n",
       "      <td>j utilise le nylon mais pas le cuir</td>\n",
       "      <td>j utilise le nylon mais pas le cuir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5833</th>\n",
       "      <td>Je n' aime pas les vélos , je préfère les Harl...</td>\n",
       "      <td>1</td>\n",
       "      <td>Je n aime pas les vélos  je préfère les Harley...</td>\n",
       "      <td>je n aime pas les vélos je préfère les harleyd...</td>\n",
       "      <td>je n aime pas les vélos je préfère les harleyd...</td>\n",
       "      <td>je n aime pas les vélos je préfère les harleyd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5834</th>\n",
       "      <td>J' aime les westerns , mais pas les jeux de so...</td>\n",
       "      <td>1</td>\n",
       "      <td>J aime les westerns  mais pas les jeux de soci...</td>\n",
       "      <td>j aime les westerns mais pas les jeux de société</td>\n",
       "      <td>j aime les westerns mais pas les jeux de société</td>\n",
       "      <td>j aime les westerns mais pas les jeux de société</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5835</th>\n",
       "      <td>Je n' aime pas les vélos , je préfère les endu...</td>\n",
       "      <td>1</td>\n",
       "      <td>Je n aime pas les vélos  je préfère les enduros</td>\n",
       "      <td>je n aime pas les vélos je préfère les enduros</td>\n",
       "      <td>je n aime pas les vélos je préfère les enduros</td>\n",
       "      <td>je n aime pas les vélos je préfère les enduros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5836</th>\n",
       "      <td>J' aime les motos , et aussi les navaires .</td>\n",
       "      <td>1</td>\n",
       "      <td>J aime les motos  et aussi les navaires</td>\n",
       "      <td>j aime les motos et aussi les navaires</td>\n",
       "      <td>j aime les motos et aussi les navaires</td>\n",
       "      <td>j aime les motos et aussi les navaires</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5837 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence  Labels  \\\n",
       "0             J' aime la dinde , et aussi les huîtres .       1   \n",
       "1     J' ai rencontré les astronomes , sauf les gref...       0   \n",
       "2     J' apprécie les armoires , et plus particulièr...       0   \n",
       "3     Il n' aime pas les émotions , il préfère la lo...       1   \n",
       "4         J' aime les pantalons , sauf les chaussures .       0   \n",
       "...                                                 ...     ...   \n",
       "5832           J' utilise le nylon , mais pas le cuir .       1   \n",
       "5833  Je n' aime pas les vélos , je préfère les Harl...       1   \n",
       "5834  J' aime les westerns , mais pas les jeux de so...       1   \n",
       "5835  Je n' aime pas les vélos , je préfère les endu...       1   \n",
       "5836        J' aime les motos , et aussi les navaires .       1   \n",
       "\n",
       "                                      Sentence_no_punct  \\\n",
       "0                J aime la dinde  et aussi les huîtres    \n",
       "1     J ai rencontré les astronomes  sauf les greffi...   \n",
       "2     J apprécie les armoires  et plus particulièrem...   \n",
       "3     Il n aime pas les émotions  il préfère la logi...   \n",
       "4            J aime les pantalons  sauf les chaussures    \n",
       "...                                                 ...   \n",
       "5832              J utilise le nylon  mais pas le cuir    \n",
       "5833  Je n aime pas les vélos  je préfère les Harley...   \n",
       "5834  J aime les westerns  mais pas les jeux de soci...   \n",
       "5835   Je n aime pas les vélos  je préfère les enduros    \n",
       "5836           J aime les motos  et aussi les navaires    \n",
       "\n",
       "                                     Sentence_tokenized  \\\n",
       "0                 j aime la dinde et aussi les huîtres    \n",
       "1     j ai rencontré les astronomes sauf les greffiers    \n",
       "2     j apprécie les armoires et plus particulièreme...   \n",
       "3     il n aime pas les émotions il préfère la logique    \n",
       "4             j aime les pantalons sauf les chaussures    \n",
       "...                                                 ...   \n",
       "5832               j utilise le nylon mais pas le cuir    \n",
       "5833  je n aime pas les vélos je préfère les harleyd...   \n",
       "5834  j aime les westerns mais pas les jeux de société    \n",
       "5835    je n aime pas les vélos je préfère les enduros    \n",
       "5836            j aime les motos et aussi les navaires    \n",
       "\n",
       "                                    Sentence_lemmatized  \\\n",
       "0                 j aime la dinde et aussi les huîtres    \n",
       "1     j ai rencontré les astronomes sauf les greffiers    \n",
       "2     j apprécie les armoires et plus particulièreme...   \n",
       "3     il n aime pas les émotions il préfère la logique    \n",
       "4             j aime les pantalons sauf les chaussures    \n",
       "...                                                 ...   \n",
       "5832               j utilise le nylon mais pas le cuir    \n",
       "5833  je n aime pas les vélos je préfère les harleyd...   \n",
       "5834  j aime les westerns mais pas les jeux de société    \n",
       "5835    je n aime pas les vélos je préfère les enduros    \n",
       "5836            j aime les motos et aussi les navaires    \n",
       "\n",
       "                                       Sentence_stemmed  \n",
       "0                 j aime la dinde et aussi les huîtres   \n",
       "1     j ai rencontré les astronomes sauf les greffiers   \n",
       "2     j apprécie les armoires et plus particulièreme...  \n",
       "3     il n aime pas les émotions il préfère la logique   \n",
       "4             j aime les pantalons sauf les chaussures   \n",
       "...                                                 ...  \n",
       "5832               j utilise le nylon mais pas le cuir   \n",
       "5833  je n aime pas les vélos je préfère les harleyd...  \n",
       "5834  j aime les westerns mais pas les jeux de société   \n",
       "5835    je n aime pas les vélos je préfère les enduros   \n",
       "5836            j aime les motos et aussi les navaires   \n",
       "\n",
       "[5837 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_concat_fr['Sentence_no_punct'] = train_concat_fr['Sentence'].apply(lambda x: remove_punct(x))\n",
    "train_concat_fr['Sentence_tokenized'] = train_concat_fr['Sentence_no_punct'].apply(lambda x: tokenize(x))\n",
    "train_concat_fr['Sentence_lemmatized'] = train_concat_fr['Sentence_tokenized'].apply(lambda x: lemmatizing(x))\n",
    "train_concat_fr['Sentence_stemmed'] = train_concat_fr['Sentence_tokenized'].apply(lambda x: stemming_fr(x))\n",
    "train_concat_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bcfb297",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fr['Sentence_no_punct'] = test_fr['Sentence'].apply(lambda x: remove_punct(x))\n",
    "test_fr['Sentence_tokenized'] = test_fr['Sentence_no_punct'].apply(lambda x: tokenize(x))\n",
    "test_fr['Sentence_lemmatized'] = test_fr['Sentence_tokenized'].apply(lambda x: lemmatizing(x))\n",
    "test_fr['Sentence_stemmed'] = test_fr['Sentence_tokenized'].apply(lambda x: stemming_fr(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47129da",
   "metadata": {},
   "source": [
    "Italian data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "403300c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Labels</th>\n",
       "      <th>Sentence_no_punct</th>\n",
       "      <th>Sentence_tokenized</th>\n",
       "      <th>Sentence_lemmatized</th>\n",
       "      <th>Sentence_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amo i cartoni animati , ma non i sussidiari .</td>\n",
       "      <td>1</td>\n",
       "      <td>Amo i cartoni animati  ma non i sussidiari</td>\n",
       "      <td>amo i cartoni animati ma non i sussidiari</td>\n",
       "      <td>amo i cartoni animati ma non i sussidiari</td>\n",
       "      <td>amo i cartoni animati ma non i sussidiari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apprezzo il vino , ma non il Chianti .</td>\n",
       "      <td>1</td>\n",
       "      <td>Apprezzo il vino  ma non il Chianti</td>\n",
       "      <td>apprezzo il vino ma non il chianti</td>\n",
       "      <td>apprezzo il vino ma non il chianti</td>\n",
       "      <td>apprezzo il vino ma non il chianti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amo i manuali , un tipo interessante di dipinto .</td>\n",
       "      <td>0</td>\n",
       "      <td>Amo i manuali  un tipo interessante di dipinto</td>\n",
       "      <td>amo i manuali un tipo interessante di dipinto</td>\n",
       "      <td>amo i manuali un tipo interessante di dipinto</td>\n",
       "      <td>amo i manuali un tipo interessante di dipinto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amo gli arbusti , ed anche le querce .</td>\n",
       "      <td>1</td>\n",
       "      <td>Amo gli arbusti  ed anche le querce</td>\n",
       "      <td>amo gli arbusti ed anche le querce</td>\n",
       "      <td>amo gli arbusti ed anche le querce</td>\n",
       "      <td>amo gli arbusti ed anche le querce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Uso il poliestere , eccetto il vetro .</td>\n",
       "      <td>0</td>\n",
       "      <td>Uso il poliestere  eccetto il vetro</td>\n",
       "      <td>uso il poliestere eccetto il vetro</td>\n",
       "      <td>uso il poliestere eccetto il vetro</td>\n",
       "      <td>uso il poliestere eccetto il vetro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Labels  \\\n",
       "0      Amo i cartoni animati , ma non i sussidiari .       1   \n",
       "1             Apprezzo il vino , ma non il Chianti .       1   \n",
       "2  Amo i manuali , un tipo interessante di dipinto .       0   \n",
       "3             Amo gli arbusti , ed anche le querce .       1   \n",
       "4             Uso il poliestere , eccetto il vetro .       0   \n",
       "\n",
       "                                 Sentence_no_punct  \\\n",
       "0      Amo i cartoni animati  ma non i sussidiari    \n",
       "1             Apprezzo il vino  ma non il Chianti    \n",
       "2  Amo i manuali  un tipo interessante di dipinto    \n",
       "3             Amo gli arbusti  ed anche le querce    \n",
       "4             Uso il poliestere  eccetto il vetro    \n",
       "\n",
       "                               Sentence_tokenized  \\\n",
       "0      amo i cartoni animati ma non i sussidiari    \n",
       "1             apprezzo il vino ma non il chianti    \n",
       "2  amo i manuali un tipo interessante di dipinto    \n",
       "3             amo gli arbusti ed anche le querce    \n",
       "4             uso il poliestere eccetto il vetro    \n",
       "\n",
       "                              Sentence_lemmatized  \\\n",
       "0      amo i cartoni animati ma non i sussidiari    \n",
       "1             apprezzo il vino ma non il chianti    \n",
       "2  amo i manuali un tipo interessante di dipinto    \n",
       "3             amo gli arbusti ed anche le querce    \n",
       "4             uso il poliestere eccetto il vetro    \n",
       "\n",
       "                                 Sentence_stemmed  \n",
       "0      amo i cartoni animati ma non i sussidiari   \n",
       "1             apprezzo il vino ma non il chianti   \n",
       "2  amo i manuali un tipo interessante di dipinto   \n",
       "3             amo gli arbusti ed anche le querce   \n",
       "4             uso il poliestere eccetto il vetro   "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_concat_it['Sentence_no_punct'] = train_concat_it['Sentence'].apply(lambda x: remove_punct(x))\n",
    "train_concat_it['Sentence_tokenized'] = train_concat_it['Sentence_no_punct'].apply(lambda x: tokenize(x))\n",
    "train_concat_it['Sentence_lemmatized'] = train_concat_it['Sentence_tokenized'].apply(lambda x: lemmatizing(x))\n",
    "train_concat_it['Sentence_stemmed'] = train_concat_it['Sentence_tokenized'].apply(lambda x: stemming_it(x))\n",
    "train_concat_it.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93abe760",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_it['Sentence_no_punct'] = test_it['Sentence'].apply(lambda x: remove_punct(x))\n",
    "test_it['Sentence_tokenized'] = test_it['Sentence_no_punct'].apply(lambda x: tokenize(x))\n",
    "test_it['Sentence_lemmatized'] = test_it['Sentence_tokenized'].apply(lambda x: lemmatizing(x))\n",
    "test_it['Sentence_stemmed'] = test_it['Sentence_tokenized'].apply(lambda x: stemming_it(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc438ea8",
   "metadata": {},
   "source": [
    "# SVC classifiers \n",
    "SVC and NuSVC are similar methods, but accept slightly different sets of parameters and have different mathematical formulations (see section Mathematical formulation). On the other hand, LinearSVC is another (faster) implementation of Support Vector Classification for the case of a linear kernel. Note that LinearSVC does not accept parameter kernel, as this is assumed to be linear. It also lacks some of the attributes of SVC and NuSVC, like support_.\n",
    "\n",
    "As other classifiers, SVC, NuSVC and LinearSVC take as input two arrays: an array X of shape (n_samples, n_features) holding the training samples, and an array y of class labels (strings or integers), of shape (n_samples).\n",
    "\n",
    "\n",
    "Source: https://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07aa906e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English data\n",
      "NuSVC\n",
      "Results of the model: \n",
      "Precision                                   : 0.645\n",
      "Recall                                      : 0.463\n",
      "F1-Score                                    : 0.539\n",
      "English data\n",
      "SVC\n",
      "Results of the model: \n",
      "Precision                                   : 0.647\n",
      "Recall                                      : 0.470\n",
      "F1-Score                                    : 0.545\n",
      "English data\n",
      "RandomForestClassifier\n",
      "Results of the model: \n",
      "Precision                                   : 0.545\n",
      "Recall                                      : 0.672\n",
      "F1-Score                                    : 0.602\n",
      "English data\n",
      "GradientBoostingClassifier\n",
      "Results of the model: \n",
      "Precision                                   : 0.665\n",
      "Recall                                      : 0.600\n",
      "F1-Score                                    : 0.631\n",
      "English data\n",
      "AdaBoostClassifier\n",
      "Results of the model: \n",
      "Precision                                   : 0.517\n",
      "Recall                                      : 0.979\n",
      "F1-Score                                    : 0.676\n",
      "English data\n",
      "DecisionTreeClassifier\n",
      "Results of the model: \n",
      "Precision                                   : 0.665\n",
      "Recall                                      : 0.600\n",
      "F1-Score                                    : 0.631\n",
      "English data\n",
      "KNeighborsClassifier\n",
      "Results of the model: \n",
      "Precision                                   : 0.736\n",
      "Recall                                      : 0.669\n",
      "F1-Score                                    : 0.701\n",
      "All models have been trained\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "#English data.\n",
    "classifiers = [  \n",
    "    NuSVC( nu=0.4, kernel='rbf', degree=3, gamma='scale',\n",
    "          coef0=0.0, shrinking=True, probability=False,tol=0.001, \n",
    "          cache_size=200, class_weight=None, verbose=False, max_iter=- 1,\n",
    "          decision_function_shape='ovr', break_ties=False, random_state=100),\n",
    "    # #Nu-Support Vector Classification uses a parameter to control the number of support vectors.\n",
    "    #The \"probability\" parameter is not contributing much to our model,the result roughly remain the same.\n",
    "    #The \"NU\" parameter,which is An upper bound on the fraction of margin errors (see User Guide) \n",
    "    #and a lower bound of the fraction of support vectors,\n",
    "    #affects the Recall a lot and the most suitable value is 0.4 and it becomes worse with incresasing.\n",
    "   \n",
    "    \n",
    "    SVC(C=0.2, kernel='rbf', degree=6, gamma='scale', coef0=0.1, shrinking=True, probability=False, \n",
    "        tol=0.1, cache_size=200, class_weight=None, verbose=False, max_iter=-1,\n",
    "        decision_function_shape='ovr', break_ties=False, random_state=42),\n",
    "   #We are implementing non-linear SVC here and it is showing the results which are worse than Linear SVC.\n",
    "   #It is observed that the most optimal value for \"C\" is 0.1 and the value \"1\" affects F-1 score in a bad way.\n",
    "    \n",
    "    \n",
    "    RandomForestClassifier(n_estimators=20, criterion='gini', max_depth=None, min_samples_split=3, min_samples_leaf=1, \n",
    "                           min_weight_fraction_leaf=0.01, max_features='auto', max_leaf_nodes=None, \n",
    "                           min_impurity_decrease=0.0, bootstrap=True, oob_score=False, \n",
    "                           n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None,\n",
    "                           ccp_alpha=0.0, max_samples=None),\n",
    "    #by raising the number of estimators we will get better results for Precision,but for the Recall goes down and the most suitable value is 20.At the value 100 Precision is 68 and Recall is 38.\n",
    "    #Save to leave criterion as \"gini\",because F-1 score goes down with \"enrtopy\"\n",
    "    #when the minimum weighted fraction of the sum total of weights decreased to 0.01 F-1 score is increased from 61 to 67.\n",
    "    #The minimum number of samples has a great ipmact on Recall:when the paramter was increased to 3,Recall went to 90.\n",
    "   \n",
    "    \n",
    "    GradientBoostingClassifier( loss='exponential', learning_rate=0.001, n_estimators=100, subsample=1.0, criterion='friedman_mse',\n",
    "                               min_samples_split=2,min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3,\n",
    "                               min_impurity_decrease=0.1, init=None, random_state=None,max_features=None, \n",
    "                               verbose=0, max_leaf_nodes=10, warm_start=False, validation_fraction=0.01, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.1),\n",
    "    \n",
    "    #when the optimization of the loss function is \"exponential\",the result is increased,but by a very little number.\n",
    "    #Increasing of the learning rate influences Recall  in a good way,but Precision plumments dramatically.\n",
    "    \n",
    "    AdaBoostClassifier(base_estimator= None,n_estimators=100, learning_rate=0.1, algorithm='SAMME', random_state=10),\n",
    "    #Number of estimators play a role and boots the Precision for  40 to 55,and Recall from 50 to 88.\n",
    "    #the learning rate of 0.01 is showing much worse results than \"1.0\",howver the most suitable learning rate value is 0.1:it boosted Recall from 88 to 97. \n",
    "    #Random state doe not have an effect on the model.\n",
    "    \n",
    "   \n",
    "    DecisionTreeClassifier( criterion='gini', splitter='best', max_depth=300, min_samples_split=2, \n",
    "                           min_samples_leaf=1,min_weight_fraction_leaf=0.1, max_features=None,random_state=None, \n",
    "                           max_leaf_nodes=None, min_impurity_decrease=0.0),\n",
    "    #The model with criterion \"entropy\" is showing  slighlty worse results.\n",
    "    #\"Max-depth\" parameter becomes worse after incresing the value of it.\n",
    "    #Other paramters should not be changed as they make the model show worse results.\n",
    "  \n",
    "    KNeighborsClassifier(n_neighbors=10, weights='uniform', algorithm='auto',\n",
    "                         leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None),\n",
    "    #K-nearest neighbors model shows the best results when number of neighbors  is equal to 10.\n",
    "    #Even when we go to the score '12',we can see some changes in Precision and F-1 Score.\n",
    "    #Moreover,when the leaf side above 30 the model is showing worse results.   \n",
    "\n",
    "    ]\n",
    "for classsif in classifiers:\n",
    "    classsif.fit(train_x_feat_en,y_train_en)\n",
    "    name = classsif.__class__.__name__\n",
    "    predicted_y_en= classsif.predict(test_x_feat_en)\n",
    "    print(\"English data\")\n",
    "    print(name)\n",
    "    print('Results of the model: ')\n",
    "    print('Precision                                   : %.3f'%precision_score(test_true_y_en,predicted_y_en))\n",
    "    print('Recall                                      : %.3f'%recall_score(test_true_y_en,predicted_y_en))\n",
    "    print('F1-Score                                    : %.3f'%f1_score(test_true_y_en,predicted_y_en))\n",
    "    \n",
    "print(\"All models have been trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c93adf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French data\n",
      "NuSVC\n",
      "Results of the model: \n",
      "Precision                                   : 0.473\n",
      "Recall                                      : 0.771\n",
      "F1-Score                                    : 0.586\n",
      "French data\n",
      "SVC\n",
      "Results of the model: \n",
      "Precision                                   : 0.473\n",
      "Recall                                      : 0.771\n",
      "F1-Score                                    : 0.586\n",
      "French data\n",
      "RandomForestClassifier\n",
      "Results of the model: \n",
      "Precision                                   : 0.471\n",
      "Recall                                      : 1.000\n",
      "F1-Score                                    : 0.640\n",
      "French data\n",
      "GradientBoostingClassifier\n",
      "Results of the model: \n",
      "Precision                                   : 0.471\n",
      "Recall                                      : 1.000\n",
      "F1-Score                                    : 0.640\n",
      "French data\n",
      "AdaBoostClassifier\n",
      "Results of the model: \n",
      "Precision                                   : 0.471\n",
      "Recall                                      : 1.000\n",
      "F1-Score                                    : 0.640\n",
      "French data\n",
      "DecisionTreeClassifier\n",
      "Results of the model: \n",
      "Precision                                   : 0.433\n",
      "Recall                                      : 0.033\n",
      "F1-Score                                    : 0.062\n",
      "French data\n",
      "KNeighborsClassifier\n",
      "Results of the model: \n",
      "Precision                                   : 0.478\n",
      "Recall                                      : 0.051\n",
      "F1-Score                                    : 0.092\n",
      "All models have been trained\n"
     ]
    }
   ],
   "source": [
    "#\"French Data\"\n",
    "classifiers = [  \n",
    "    NuSVC( nu=0.4, kernel='rbf', degree=3, gamma='scale',\n",
    "          coef0=0.0, shrinking=True, probability=False,tol=0.001, \n",
    "          cache_size=200, class_weight=None, verbose=False, max_iter=- 1,\n",
    "          decision_function_shape='ovr', break_ties=False, random_state=100),\n",
    "  \n",
    "    \n",
    "    SVC(C=0.2, kernel='rbf', degree=6, gamma='scale', coef0=0.1, shrinking=True, probability=False, \n",
    "        tol=0.1, cache_size=200, class_weight=None, verbose=False, max_iter=-1,\n",
    "        decision_function_shape='ovr', break_ties=False, random_state=42),\n",
    "   \n",
    "    \n",
    "    RandomForestClassifier(n_estimators=20, criterion='gini', max_depth=None, min_samples_split=3, min_samples_leaf=1, \n",
    "                           min_weight_fraction_leaf=0.01, max_features='auto', max_leaf_nodes=None, \n",
    "                           min_impurity_decrease=0.0, bootstrap=True, oob_score=False, \n",
    "                           n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None,\n",
    "                           ccp_alpha=0.0, max_samples=None),\n",
    "    \n",
    "    GradientBoostingClassifier( learning_rate=0.001, n_estimators=100, subsample=1.0, criterion='friedman_mse',\n",
    "                               min_samples_split=2,min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3,\n",
    "                               min_impurity_decrease=0.1, init=None, random_state=None,max_features=None, \n",
    "                               verbose=0, max_leaf_nodes=10, warm_start=False, validation_fraction=0.01, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.1),\n",
    "    \n",
    "    \n",
    "    AdaBoostClassifier(base_estimator= None,n_estimators=100, learning_rate=0.1, algorithm='SAMME', random_state=10),\n",
    "   \n",
    "    DecisionTreeClassifier( ),\n",
    "  \n",
    "    KNeighborsClassifier(n_neighbors=10, weights='uniform', algorithm='auto',\n",
    "                         leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None),\n",
    "    ]\n",
    "for classsif in classifiers:\n",
    "    classsif.fit(train_x_feat_fr,y_train_fr)\n",
    "    name = classsif.__class__.__name__\n",
    "    predicted_y_fr= classsif.predict(test_x_feat_fr)\n",
    "    print(\"French data\")\n",
    "    print(name)\n",
    "    print('Results of the model: ')\n",
    "    print('Precision                                   : %.3f'%precision_score(test_true_y_fr,predicted_y_fr))\n",
    "    print('Recall                                      : %.3f'%recall_score(test_true_y_fr,predicted_y_fr))\n",
    "    print('F1-Score                                    : %.3f'%f1_score(test_true_y_fr,predicted_y_fr))\n",
    "    \n",
    "print(\"All models have been trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70677719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian data\n",
      "NuSVC\n",
      "Results of the model: \n",
      "Precision                                   : 0.545\n",
      "Recall                                      : 0.593\n",
      "F1-Score                                    : 0.568\n",
      "Italian data\n",
      "SVC\n",
      "Results of the model: \n",
      "Precision                                   : 0.534\n",
      "Recall                                      : 0.768\n",
      "F1-Score                                    : 0.630\n",
      "Italian data\n",
      "RandomForestClassifier\n",
      "Results of the model: \n",
      "Precision                                   : 0.580\n",
      "Recall                                      : 0.731\n",
      "F1-Score                                    : 0.647\n",
      "Italian data\n",
      "GradientBoostingClassifier\n",
      "Results of the model: \n",
      "Precision                                   : 0.471\n",
      "Recall                                      : 1.000\n",
      "F1-Score                                    : 0.640\n",
      "Italian data\n",
      "AdaBoostClassifier\n",
      "Results of the model: \n",
      "Precision                                   : 0.517\n",
      "Recall                                      : 0.979\n",
      "F1-Score                                    : 0.676\n",
      "Italian data\n",
      "DecisionTreeClassifier\n",
      "Results of the model: \n",
      "Precision                                   : 0.389\n",
      "Recall                                      : 0.351\n",
      "F1-Score                                    : 0.369\n",
      "Italian data\n",
      "KNeighborsClassifier\n",
      "Results of the model: \n",
      "Precision                                   : 0.678\n",
      "Recall                                      : 0.636\n",
      "F1-Score                                    : 0.656\n",
      "All models have been trained\n"
     ]
    }
   ],
   "source": [
    "#\"Italian Data\"\n",
    "classifiers = [  \n",
    "    NuSVC( nu=0.4, kernel='rbf', degree=3, gamma='scale',\n",
    "          coef0=0.0, shrinking=True, probability=False,tol=0.001, \n",
    "          cache_size=200, class_weight=None, verbose=False, max_iter=- 1,\n",
    "          decision_function_shape='ovr', break_ties=False, random_state=100),\n",
    "  \n",
    "    \n",
    "    SVC(C=0.2, kernel='rbf', degree=6, gamma='scale', coef0=0.1, shrinking=True, probability=False, \n",
    "        tol=0.1, cache_size=200, class_weight=None, verbose=False, max_iter=-1,\n",
    "        decision_function_shape='ovr', break_ties=False, random_state=42),\n",
    "   \n",
    "    \n",
    "    RandomForestClassifier(n_estimators=20, criterion='gini', max_depth=None, min_samples_split=3, min_samples_leaf=1, \n",
    "                           min_weight_fraction_leaf=0.01, max_features='auto', max_leaf_nodes=None, \n",
    "                           min_impurity_decrease=0.0, bootstrap=True, oob_score=False, \n",
    "                           n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None,\n",
    "                           ccp_alpha=0.0, max_samples=None),\n",
    "    \n",
    "    GradientBoostingClassifier( learning_rate=0.001, n_estimators=100, subsample=1.0, criterion='friedman_mse',\n",
    "                               min_samples_split=2,min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3,\n",
    "                               min_impurity_decrease=0.1, init=None, random_state=None,max_features=None, \n",
    "                               verbose=0, max_leaf_nodes=10, warm_start=False, validation_fraction=0.01, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.1),\n",
    "    \n",
    "    \n",
    "    AdaBoostClassifier(base_estimator= None,n_estimators=100, learning_rate=0.1, algorithm='SAMME', random_state=10),\n",
    "   \n",
    "    DecisionTreeClassifier( ),\n",
    "  \n",
    "    KNeighborsClassifier(n_neighbors=10, weights='uniform', algorithm='auto',\n",
    "                         leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)]\n",
    "for classsif in classifiers:\n",
    "    classsif.fit(train_x_feat_it,y_train_it)\n",
    "    name = classsif.__class__.__name__\n",
    "    predicted_y_it= classsif.predict(test_x_feat_it)\n",
    "    print(\"Italian data\")\n",
    "    print(name)\n",
    "    print('Results of the model: ')\n",
    "    print('Precision                                   : %.3f'%precision_score(test_true_y_it,predicted_y_it))\n",
    "    print('Recall                                      : %.3f'%recall_score(test_true_y_it,predicted_y_it))\n",
    "    print('F1-Score                                    : %.3f'%f1_score(test_true_y_it,predicted_y_it))\n",
    "    \n",
    "print(\"All models have been trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5518c5e",
   "metadata": {},
   "source": [
    "# Testing with preprocced data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f69a8c6",
   "metadata": {},
   "source": [
    "Enlgish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8163d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Results for English data with lemmatization ==========\n",
      "Precision                                   : 0.663\n",
      "Recall                                      : 0.890\n",
      "F1-Score                                    : 0.760\n"
     ]
    }
   ],
   "source": [
    "\"Goal: With some minor changes it the classifier check the difference between raw  and stemmed data\"\n",
    "\n",
    "train_texts_en_pr=(train_concat_en['Sentence_stemmed'])\n",
    "y_train_en_pr=np.array(train_concat_en['Labels'])\n",
    "dev_texts_en_pr=(test_en['Sentence_stemmed'])\n",
    "test_true_y_en_pr=np.array(test_en['Labels'])   \n",
    "#Vectorization_enlgish.\n",
    "tfidf_vectorizer = TfidfVectorizer(encoding='utf-8', lowercase=True, ngram_range=(1, 3),\n",
    "                                   norm='l1',use_idf=True, max_df=0.70, min_df=3, max_features=None) \n",
    "train_x_feat_en_pr= tfidf_vectorizer.fit_transform(train_texts_en_pr)\n",
    "classifier_svc_en_pr= LinearSVC(random_state=42, C=1, class_weight='balanced', tol=0.0001).fit(train_x_feat_en_pr,                                             \n",
    "                                                                                              y_train_en_pr)\n",
    "##Testing_english:\n",
    "test_x_feat_en_pr= tfidf_vectorizer.transform(dev_texts_en_pr)\n",
    "test_pred_y_en_pr= classifier_svc_en_pr.predict(test_x_feat_en_pr)\n",
    "print(\"=\"*10+\" Results for English data with lemmatization \"+\"=\"*10)\n",
    "print('Precision                                   : %.3f'%precision_score(test_true_y_en_pr, test_pred_y_en_pr))\n",
    "print('Recall                                      : %.3f'%recall_score(test_true_y_en_pr, test_pred_y_en_pr))\n",
    "print('F1-Score                                    : %.3f'%f1_score(test_true_y_en_pr, test_pred_y_en_pr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1df892a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Results for English data with another type of vecotorization ==========\n",
      "Precision                                   : 0.620\n",
      "Recall                                      : 0.693\n",
      "F1-Score                                    : 0.654\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "train_texts_en_pr=(train_concat_en['Sentence_stemmed'])\n",
    "y_train_en_pr=np.array(train_concat_en['Labels'])\n",
    "dev_texts_en_pr=(test_en['Sentence_stemmed'])\n",
    "test_true_y_en_pr=np.array(test_en['Labels'])   \n",
    "#Vectorization_enlgish.\n",
    "hashing_vectorizer= HashingVectorizer(n_features=2**4)\n",
    "train_x_feat_en_pr1= hashing_vectorizer.fit_transform(train_texts_en_pr)\n",
    "classifier_svc_en_pr= LinearSVC(random_state=42, C=1, class_weight='balanced', tol=0.0001).fit(train_x_feat_en_pr1,                                             \n",
    "                                                                                              y_train_en_pr)\n",
    "##Testing_english:\n",
    "test_x_feat_en_pr1= hashing_vectorizer.transform(dev_texts_en_pr)\n",
    "test_pred_y_en_pr= classifier_svc_en_pr.predict(test_x_feat_en_pr1)\n",
    "print(\"=\"*10+\" Results for English data with another type of vecotorization \"+\"=\"*10)\n",
    "print('Precision                                   : %.3f'%precision_score(test_true_y_en_pr, test_pred_y_en_pr))\n",
    "print('Recall                                      : %.3f'%recall_score(test_true_y_en_pr, test_pred_y_en_pr))\n",
    "print('F1-Score                                    : %.3f'%f1_score(test_true_y_en_pr, test_pred_y_en_pr))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21f41ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Results for English data with stemming ==========\n",
      "Precision                                   : 0.637\n",
      "Recall                                      : 0.812\n",
      "F1-Score                                    : 0.714\n"
     ]
    }
   ],
   "source": [
    "#With another type of a vecorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "train_texts_en_pr=(train_concat_en['Sentence_stemmed'])\n",
    "y_train_en_pr=np.array(train_concat_en['Labels'])\n",
    "dev_texts_en_pr=(test_en['Sentence_stemmed'])\n",
    "test_true_y_en_pr=np.array(test_en['Labels'])   \n",
    "#Vectorization_enlgish.\n",
    "Count_vectorizer = CountVectorizer()\n",
    "\n",
    "train_x_feat_en_pr2= Count_vectorizer.fit_transform(train_texts_en_pr)\n",
    "classifier_svc_en_pr= LinearSVC(random_state=42, C=1, class_weight='balanced', tol=0.0001).fit(train_x_feat_en_pr2,                                             \n",
    "                                                                                              y_train_en_pr)\n",
    "##Testing_english:\n",
    "test_x_feat_en_pr2= Count_vectorizer.transform(dev_texts_en_pr)\n",
    "test_pred_y_en_pr2= classifier_svc_en_pr.predict(test_x_feat_en_pr2)\n",
    "print(\"=\"*10+\" Results for English data with stemming \"+\"=\"*10)\n",
    "print('Precision                                   : %.3f'%precision_score(test_true_y_en_pr, test_pred_y_en_pr2))\n",
    "print('Recall                                      : %.3f'%recall_score(test_true_y_en_pr, test_pred_y_en_pr2))\n",
    "print('F1-Score                                    : %.3f'%f1_score(test_true_y_en_pr, test_pred_y_en_pr2))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6996190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preporcessed with KNeighbors_Classifier \n",
      "Results of the model: \n",
      "Precision                                   : 0.613\n",
      "Recall                                      : 0.758\n",
      "F1-Score                                    : 0.678\n"
     ]
    }
   ],
   "source": [
    "#Experiment1\n",
    "KNeighbors_Classifier=KNeighborsClassifier(n_neighbors=10, weights='uniform', algorithm='auto',\n",
    "                    leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
    "\n",
    "KNeighbors_Classifier.fit(train_x_feat_en_pr,y_train_en_pr)\n",
    "predicted_y_en_pr= KNeighbors_Classifier.predict(test_x_feat_en_pr)\n",
    "print(\"Preporcessed with KNeighbors_Classifier \")\n",
    "print('Results of the model: ')\n",
    "print('Precision                                   : %.3f'%precision_score(test_true_y_en_pr,predicted_y_en_pr))\n",
    "print('Recall                                      : %.3f'%recall_score(test_true_y_en_pr,predicted_y_en_pr))\n",
    "print('F1-Score                                    : %.3f'%f1_score(test_true_y_en_pr,predicted_y_en_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47852a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preporcessed with KNeighbors_Classifier \n",
      "Results of the model: \n",
      "Precision                                   : 0.603\n",
      "Recall                                      : 0.764\n",
      "F1-Score                                    : 0.674\n"
     ]
    }
   ],
   "source": [
    "#Experiment2\n",
    "KNeighbors_Classifier=KNeighborsClassifier(n_neighbors=10, weights='distance', algorithm='auto',\n",
    "                    leaf_size=30, p=1, metric='minkowski', metric_params=None, n_jobs=None)\n",
    "\n",
    "KNeighbors_Classifier.fit(train_x_feat_en_pr,y_train_en_pr)\n",
    "predicted_y_en_pr= KNeighbors_Classifier.predict(test_x_feat_en_pr)\n",
    "print(\"Preporcessed with KNeighbors_Classifier \")\n",
    "print('Results of the model: ')\n",
    "print('Precision                                   : %.3f'%precision_score(test_true_y_en_pr,predicted_y_en_pr))\n",
    "print('Recall                                      : %.3f'%recall_score(test_true_y_en_pr,predicted_y_en_pr))\n",
    "print('F1-Score                                    : %.3f'%f1_score(test_true_y_en_pr,predicted_y_en_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3026f8",
   "metadata": {},
   "source": [
    "French data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44091386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Results for French data with stemming ==========\n",
      "Precision                                   : 0.650\n",
      "Recall                                      : 0.899\n",
      "F1-Score                                    : 0.754\n"
     ]
    }
   ],
   "source": [
    "\"Goal : to see the change  between pre-processeed French data and raw French data\"\n",
    "\n",
    "train_texts_fr_pr=(train_concat_fr['Sentence_stemmed'])\n",
    "y_train_fr_pr=np.array(train_concat_fr['Labels'])\n",
    "dev_texts_fr_pr=(test_fr['Sentence_stemmed'])\n",
    "test_true_y_fr_pr=np.array(test_fr['Labels'])   \n",
    "#Vectorization.\n",
    "train_x_feat_fr_pr= tfidf_vectorizer.fit_transform(train_texts_fr_pr)\n",
    "classifier_svc_fr_pr= LinearSVC(random_state=42, C=1, class_weight='balanced', tol=0.0001).fit(train_x_feat_fr_pr,                                             \n",
    "                                                                                              y_train_fr_pr)\n",
    "##Testing_english:\n",
    "test_x_feat_fr_pr= tfidf_vectorizer.transform(dev_texts_fr_pr)\n",
    "test_pred_y_fr_pr= classifier_svc_fr_pr.predict(test_x_feat_fr_pr)\n",
    "print(\"=\"*10+\" Results for French data with stemming \"+\"=\"*10)\n",
    "print('Precision                                   : %.3f'%precision_score(test_true_y_fr_pr, test_pred_y_fr_pr))\n",
    "print('Recall                                      : %.3f'%recall_score(test_true_y_fr_pr, test_pred_y_fr_pr))\n",
    "print('F1-Score                                    : %.3f'%f1_score(test_true_y_fr_pr, test_pred_y_fr_pr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7dda8729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preporcessed with NuSVC_classifier \n",
      "Results of the model: \n",
      "Precision                                   : 0.783\n",
      "Recall                                      : 0.528\n",
      "F1-Score                                    : 0.630\n"
     ]
    }
   ],
   "source": [
    "#Experiment 1.\n",
    "NuSVC_classifier=NuSVC( nu=0.1, kernel='rbf', degree=3, gamma='scale',coef0=0.0, shrinking=True, \n",
    "                       probability=False,tol=0.001, cache_size=200, class_weight=None, verbose=False,\n",
    "                       max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=100)\n",
    "NuSVC_classifier.fit(train_x_feat_fr_pr,y_train_fr_pr)\n",
    "predicted_y_fr_pr= NuSVC_classifier.predict(test_x_feat_fr_pr)\n",
    "print(\"Preporcessed with NuSVC_classifier \")\n",
    "print('Results of the model: ')\n",
    "print('Precision                                   : %.3f'%precision_score(test_true_y_fr_pr, predicted_y_fr_pr))\n",
    "print('Recall                                      : %.3f'%recall_score(test_true_y_fr_pr, predicted_y_fr_pr))\n",
    "print('F1-Score                                    : %.3f'%f1_score(test_true_y_fr_pr, predicted_y_fr_pr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e9b4c1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preporcessed with NuSVC_classifier \n",
      "Results of the model: \n",
      "Precision                                   : 0.818\n",
      "Recall                                      : 0.496\n",
      "F1-Score                                    : 0.618\n"
     ]
    }
   ],
   "source": [
    "#Experiment 2.\n",
    "NuSVC_classifier=NuSVC( nu=0.5, kernel='rbf', degree=3, gamma='scale',coef0=0.0, shrinking=True, \n",
    "                       probability=False,tol=0.001, cache_size=200, class_weight=None, verbose=False,\n",
    "                       max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=100)\n",
    "NuSVC_classifier.fit(train_x_feat_fr_pr,y_train_fr_pr)\n",
    "predicted_y_fr_pr= NuSVC_classifier.predict(test_x_feat_fr_pr)\n",
    "print(\"Preporcessed with NuSVC_classifier \")\n",
    "print('Results of the model: ')\n",
    "print('Precision                                   : %.3f'%precision_score(test_true_y_fr_pr, predicted_y_fr_pr))\n",
    "print('Recall                                      : %.3f'%recall_score(test_true_y_fr_pr, predicted_y_fr_pr))\n",
    "print('F1-Score                                    : %.3f'%f1_score(test_true_y_fr_pr, predicted_y_fr_pr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "669f68e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preporcessed with NuSVC_classifier \n",
      "Results of the model: \n",
      "Precision                                   : 0.860\n",
      "Recall                                      : 0.498\n",
      "F1-Score                                    : 0.630\n"
     ]
    }
   ],
   "source": [
    "#Experiment 3.\n",
    "NuSVC_classifier=NuSVC( nu=0.3, kernel='rbf', degree=3, gamma='scale',coef0=0.0, shrinking=True, \n",
    "                       probability=False,tol=0.001, cache_size=200, class_weight=None, verbose=False,\n",
    "                       max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=100)\n",
    "NuSVC_classifier.fit(train_x_feat_fr_pr,y_train_fr_pr)\n",
    "predicted_y_fr_pr= NuSVC_classifier.predict(test_x_feat_fr_pr)\n",
    "print(\"Preporcessed with NuSVC_classifier \")\n",
    "print('Results of the model: ')\n",
    "print('Precision                                   : %.3f'%precision_score(test_true_y_fr_pr, predicted_y_fr_pr))\n",
    "print('Recall                                      : %.3f'%recall_score(test_true_y_fr_pr, predicted_y_fr_pr))\n",
    "print('F1-Score                                    : %.3f'%f1_score(test_true_y_fr_pr, predicted_y_fr_pr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180b7a8",
   "metadata": {},
   "source": [
    "Italian data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5792ee7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Results for French  with pre-processed data==========\n",
      "Precision                                   : 0.492\n",
      "Recall                                      : 0.683\n",
      "F1-Score                                    : 0.572\n"
     ]
    }
   ],
   "source": [
    "\"Goal : to see the change  between pre-processeed Italian data and raw Italian data\"\n",
    "\n",
    "train_texts_it_pr=(train_concat_it['Sentence_stemmed'])\n",
    "y_train_it_pr=np.array(train_concat_it['Labels'])\n",
    "dev_texts_it_pr=(test_it['Sentence_stemmed'])\n",
    "test_true_y_it_pr=np.array(test_it['Labels'])   \n",
    "#Vectorization.\n",
    "train_x_feat_it_pr= tfidf_vectorizer.fit_transform(train_texts_it_pr)\n",
    "classifier_svc_it_pr= LinearSVC(random_state=42, C=1, class_weight='balanced', tol=0.0001).fit(train_x_feat_it_pr,                                             \n",
    "                                                                                              y_train_it_pr)\n",
    "##Testing_italian:\n",
    "test_x_feat_it_pr= tfidf_vectorizer.transform(dev_texts_it_pr)\n",
    "test_pred_y_it_pr= classifier_svc_fr_pr.predict(test_x_feat_it_pr)\n",
    "print(\"=\"*10+\" Results for French  with pre-processed data\"+\"=\"*10)\n",
    "print('Precision                                   : %.3f'%precision_score(test_true_y_it_pr, test_pred_y_it_pr))\n",
    "print('Recall                                      : %.3f'%recall_score(test_true_y_it_pr, test_pred_y_it_pr))\n",
    "print('F1-Score                                    : %.3f'%f1_score(test_true_y_it_pr, test_pred_y_it_pr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7776cad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Results for Italian data with another type of vecotorization ==========\n",
      "Precision                                   : 0.593\n",
      "Recall                                      : 0.683\n",
      "F1-Score                                    : 0.635\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "train_texts_it_pr=(train_concat_it['Sentence_stemmed'])\n",
    "y_train_it_pr=np.array(train_concat_it['Labels'])\n",
    "dev_texts_it_pr=(test_it['Sentence_stemmed'])\n",
    "test_true_y_it_pr=np.array(test_it['Labels'])   \n",
    "#Vectorization_enlgish.\n",
    "hashing_vectorizer= HashingVectorizer(n_features=2**4)\n",
    "train_x_feat_it_pr1= hashing_vectorizer.fit_transform(train_texts_it_pr)\n",
    "classifier_svc_it_pr= LinearSVC(random_state=42, C=1, class_weight='balanced', tol=0.0001).fit(train_x_feat_it_pr1,                                             \n",
    "                                                                                              y_train_it_pr)\n",
    "##Testing_italian:\n",
    "test_x_feat_it_pr1= hashing_vectorizer.transform(dev_texts_it_pr)\n",
    "test_pred_y_it_pr= classifier_svc_it_pr.predict(test_x_feat_it_pr1)\n",
    "print(\"=\"*10+\" Results for Italian data with another type of vecotorization \"+\"=\"*10)\n",
    "print('Precision                                   : %.3f'%precision_score(test_true_y_it_pr, test_pred_y_it_pr))\n",
    "print('Recall                                      : %.3f'%recall_score(test_true_y_it_pr, test_pred_y_it_pr))\n",
    "print('F1-Score                                    : %.3f'%f1_score(test_true_y_it_pr, test_pred_y_it_pr))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75407af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preporcessed with NuSVC_classifier \n",
      "Results of the model: \n",
      "Precision                                   : 0.562\n",
      "Recall                                      : 0.518\n",
      "F1-Score                                    : 0.539\n"
     ]
    }
   ],
   "source": [
    "NuSVC_classifier=NuSVC( nu=0.3, kernel='rbf', degree=3, gamma='scale',coef0=0.0, shrinking=True, \n",
    "                       probability=False,tol=0.001, cache_size=200, class_weight=None, verbose=False,\n",
    "                       max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=100)\n",
    "NuSVC_classifier.fit(train_x_feat_it_pr,y_train_it_pr)\n",
    "predicted_y_it_pr= NuSVC_classifier.predict(test_x_feat_it_pr)\n",
    "print(\"Preporcessed with NuSVC_classifier \")\n",
    "print('Results of the model: ')\n",
    "print('Precision                                   : %.3f'%precision_score(test_true_y_it_pr, predicted_y_it_pr))\n",
    "print('Recall                                      : %.3f'%recall_score(test_true_y_it_pr, predicted_y_it_pr))\n",
    "print('F1-Score                                    : %.3f'%f1_score(test_true_y_it_pr, predicted_y_it_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c907917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preporcessed with NuSVC_classifier \n",
      "Results of the model: \n",
      "Precision                                   : 0.538\n",
      "Recall                                      : 0.745\n",
      "F1-Score                                    : 0.625\n"
     ]
    }
   ],
   "source": [
    "NuSVC_classifier=NuSVC( nu=0.5, kernel='rbf', degree=3, gamma='scale',coef0=0.0, shrinking=True, \n",
    "                       probability=False,tol=0.001, cache_size=200, class_weight=None, verbose=False,\n",
    "                       max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=100)\n",
    "NuSVC_classifier.fit(train_x_feat_it_pr,y_train_it_pr)\n",
    "predicted_y_it_pr= NuSVC_classifier.predict(test_x_feat_it_pr)\n",
    "print(\"Preporcessed with NuSVC_classifier \")\n",
    "print('Results of the model: ')\n",
    "print('Precision                                   : %.3f'%precision_score(test_true_y_it_pr, predicted_y_it_pr))\n",
    "print('Recall                                      : %.3f'%recall_score(test_true_y_it_pr, predicted_y_it_pr))\n",
    "print('F1-Score                                    : %.3f'%f1_score(test_true_y_it_pr, predicted_y_it_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "655d668a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preporcessed with NuSVC_classifier \n",
      "Results of the model: \n",
      "Precision                                   : 0.523\n",
      "Recall                                      : 0.969\n",
      "F1-Score                                    : 0.680\n"
     ]
    }
   ],
   "source": [
    "NuSVC_classifier=NuSVC( nu=0.7, kernel='rbf', degree=3, gamma='scale',coef0=0.0, shrinking=True, \n",
    "                       probability=False,tol=0.001, cache_size=200, class_weight=None, verbose=False,\n",
    "                       max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=100)\n",
    "NuSVC_classifier.fit(train_x_feat_it_pr,y_train_it_pr)\n",
    "predicted_y_it_pr= NuSVC_classifier.predict(test_x_feat_it_pr)\n",
    "print(\"Preporcessed with NuSVC_classifier \")\n",
    "print('Results of the model: ')\n",
    "print('Precision                                   : %.3f'%precision_score(test_true_y_it_pr, predicted_y_it_pr))\n",
    "print('Recall                                      : %.3f'%recall_score(test_true_y_it_pr, predicted_y_it_pr))\n",
    "print('F1-Score                                    : %.3f'%f1_score(test_true_y_it_pr, predicted_y_it_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aaf75e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preporcessed with NuSVC_classifier \n",
      "Results of the model: \n",
      "Precision                                   : 0.619\n",
      "Recall                                      : 0.858\n",
      "F1-Score                                    : 0.719\n"
     ]
    }
   ],
   "source": [
    "NuSVC_classifier=NuSVC( nu=0.7, kernel='poly', degree=3, gamma='scale',coef0=0.01, shrinking=False, \n",
    "                       probability=False,tol=0.001, cache_size=200, class_weight=None, verbose=False,\n",
    "                       max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=42)\n",
    "NuSVC_classifier.fit(train_x_feat_it_pr,y_train_it_pr)\n",
    "predicted_y_it_pr= NuSVC_classifier.predict(test_x_feat_it_pr)\n",
    "print(\"Preporcessed with NuSVC_classifier \")\n",
    "print('Results of the model: ')\n",
    "print('Precision                                   : %.3f'%precision_score(test_true_y_it_pr, predicted_y_it_pr))\n",
    "print('Recall                                      : %.3f'%recall_score(test_true_y_it_pr, predicted_y_it_pr))\n",
    "print('F1-Score                                    : %.3f'%f1_score(test_true_y_it_pr, predicted_y_it_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e5a6d1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preporcessed with NuSVC_classifier \n",
      "Results of the model: \n",
      "Precision                                   : 0.637\n",
      "Recall                                      : 0.852\n",
      "F1-Score                                    : 0.729\n"
     ]
    }
   ],
   "source": [
    "NuSVC_classifier=NuSVC( nu=0.7, kernel='poly', degree=5, gamma='scale',coef0=0.01, shrinking=False, \n",
    "                       probability=False,tol=0.001, cache_size=200, class_weight=None, verbose=False,\n",
    "                       max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=42)\n",
    "NuSVC_classifier.fit(train_x_feat_it_pr,y_train_it_pr)\n",
    "predicted_y_it_pr= NuSVC_classifier.predict(test_x_feat_it_pr)\n",
    "print(\"Preporcessed with NuSVC_classifier \")\n",
    "print('Results of the model: ')\n",
    "print('Precision                                   : %.3f'%precision_score(test_true_y_it_pr, predicted_y_it_pr))\n",
    "print('Recall                                      : %.3f'%recall_score(test_true_y_it_pr, predicted_y_it_pr))\n",
    "print('F1-Score                                    : %.3f'%f1_score(test_true_y_it_pr, predicted_y_it_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "90656994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preporcessed with NuSVC_classifier \n",
      "Results of the model: \n",
      "Precision                                   : 0.637\n",
      "Recall                                      : 0.852\n",
      "F1-Score                                    : 0.729\n"
     ]
    }
   ],
   "source": [
    "NuSVC_classifier=NuSVC( nu=0.7, kernel='poly', degree=5, gamma='scale',coef0=0.001, shrinking=False, \n",
    "                       probability=False,tol=0.001, cache_size=200, class_weight=None, verbose=False,\n",
    "                       max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=42)\n",
    "NuSVC_classifier.fit(train_x_feat_it_pr,y_train_it_pr)\n",
    "predicted_y_it_pr= NuSVC_classifier.predict(test_x_feat_it_pr)\n",
    "print(\"Preporcessed with NuSVC_classifier \")\n",
    "print('Results of the model: ')\n",
    "print('Precision                                   : %.3f'%precision_score(test_true_y_it_pr, predicted_y_it_pr))\n",
    "print('Recall                                      : %.3f'%recall_score(test_true_y_it_pr, predicted_y_it_pr))\n",
    "print('F1-Score                                    : %.3f'%f1_score(test_true_y_it_pr, predicted_y_it_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd809b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
