{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b709e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "import fasttext.util\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "350cb5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_en_train_0 = 'data/train/en/En-Subtask1-fold_0.tsv'\n",
    "path_en_train_1 = 'data/train/en/En-Subtask1-fold_1.tsv'\n",
    "path_en_train_2 = 'data/train/en/En-Subtask1-fold_2.tsv'\n",
    "\n",
    "path_en_test    = 'data/test/En-Subtask1-labels.tsv'\n",
    "\n",
    "path_fr_train_0 = 'data/train/fr/Fr-Subtask1-fold_0.tsv'\n",
    "path_fr_train_1 = 'data/train/fr/Fr-Subtask1-fold_1.tsv'\n",
    "path_fr_train_2 = 'data/train/fr/Fr-Subtask1-fold_2.tsv'\n",
    "\n",
    "path_fr_test    = 'data/test/Fr-Subtask1-labels.tsv'\n",
    "\n",
    "path_it_train_0 = 'data/train/it/It-Subtask1-fold_0.tsv'\n",
    "path_it_train_1 = 'data/train/it/It-Subtask1-fold_1.tsv'\n",
    "path_it_train_2 = 'data/train/it/It-Subtask1-fold_2.tsv'\n",
    "\n",
    "path_it_test    = 'data/test/It-Subtask1-labels.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b367299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data:\n",
    "\n",
    "#EN\n",
    "#Train data \n",
    "train_data_en_0 = pd.read_table(path_en_train_0)\n",
    "train_data_en_1 = pd.read_table(path_en_train_1)\n",
    "train_data_en_2 = pd.read_table(path_en_train_2)\n",
    "#Test data\n",
    "test_data_en    = pd.read_table(path_en_test)\n",
    "\n",
    "\n",
    "#FR\n",
    "#Train data \n",
    "train_data_fr_0 = pd.read_table(path_fr_train_0)\n",
    "train_data_fr_1 = pd.read_table(path_fr_train_1)\n",
    "train_data_fr_2 = pd.read_table(path_fr_train_2)\n",
    "#Test data\n",
    "test_data_fr    = pd.read_table(path_fr_test)\n",
    "\n",
    "\n",
    "#IT\n",
    "#Train data \n",
    "train_data_it_0 = pd.read_table(path_it_train_0)\n",
    "train_data_it_1 = pd.read_table(path_it_train_1)\n",
    "train_data_it_2 = pd.read_table(path_it_train_2)\n",
    "#Test data\n",
    "test_data_it    = pd.read_table(path_it_test)\n",
    "\n",
    "#Define sentences and labels arrays:\n",
    "\n",
    "#EN-----------------------------------------------------------\n",
    "#Train data \n",
    "\n",
    "#Sentences:\n",
    "train_sentences_en_0 = train_data_en_0[['Sentence']].to_numpy()\n",
    "train_sentences_en_1 = train_data_en_1[['Sentence']].to_numpy()\n",
    "train_sentences_en_2 = train_data_en_2[['Sentence']].to_numpy()\n",
    "\n",
    "#Labels\n",
    "train_labels_en_0 = train_data_en_0['Labels'].to_numpy()\n",
    "train_labels_en_1 = train_data_en_1['Labels'].to_numpy()\n",
    "train_labels_en_2 = train_data_en_2['Labels'].to_numpy()\n",
    "\n",
    "#Test data:\n",
    "test_sentences_en = test_data_en[['Sentence']].to_numpy()\n",
    "test_labels_en    = test_data_en['Labels'].to_numpy()\n",
    "\n",
    "\n",
    "#FR-----------------------------------------------------------\n",
    "#Train data \n",
    "\n",
    "#Sentences:\n",
    "train_sentences_fr_0 = train_data_fr_0[['Sentence']].to_numpy()\n",
    "train_sentences_fr_1 = train_data_fr_1[['Sentence']].to_numpy()\n",
    "train_sentences_fr_2 = train_data_fr_2[['Sentence']].to_numpy()\n",
    "\n",
    "#Labels\n",
    "train_labels_fr_0 = train_data_fr_0['Labels'].to_numpy()\n",
    "train_labels_fr_1 = train_data_fr_1['Labels'].to_numpy()\n",
    "train_labels_fr_2 = train_data_fr_2['Labels'].to_numpy()\n",
    "\n",
    "#Test data:\n",
    "test_sentences_fr = test_data_fr[['Sentence']].to_numpy()\n",
    "test_labels_fr    = test_data_fr['Labels'].to_numpy()\n",
    "\n",
    "\n",
    "#IT-----------------------------------------------------------\n",
    "#Train data \n",
    "\n",
    "#Senetnces:\n",
    "train_sentences_it_0 = train_data_it_0[['Sentence']].to_numpy()\n",
    "train_sentences_it_1 = train_data_it_1[['Sentence']].to_numpy()\n",
    "train_sentences_it_2 = train_data_it_2[['Sentence']].to_numpy()\n",
    "\n",
    "#Labels\n",
    "train_labels_it_0 = train_data_it_0['Labels'].to_numpy()\n",
    "train_labels_it_1 = train_data_it_1['Labels'].to_numpy()\n",
    "train_labels_it_2 = train_data_it_2['Labels'].to_numpy()\n",
    "\n",
    "#Test data:\n",
    "test_sentences_it = test_data_it[['Sentence']].to_numpy()\n",
    "test_labels_it    = test_data_it['Labels'].to_numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc9ec2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get tokenizers, define tokenization function \n",
    "tokenizer_en = get_tokenizer('spacy', language=\"en_core_web_sm\")\n",
    "tokenizer_fr = get_tokenizer('spacy', language=\"fr_core_news_sm\")\n",
    "tokenizer_it = get_tokenizer('spacy', language=\"it_core_news_sm\")\n",
    "\n",
    "def tokenize(data,tokenizer):\n",
    "    tokenised_sentences = []\n",
    "    for row in data:\n",
    "        tokenised_sentences.append(tokenizer(row[0]))\n",
    "    return tokenised_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6896a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EN---------------------------\n",
    "tokenized_train_sentences_en_0 = tokenize(train_sentences_en_0, tokenizer_en)\n",
    "tokenized_train_sentences_en_1 = tokenize(train_sentences_en_1, tokenizer_en)\n",
    "tokenized_train_sentences_en_2 = tokenize(train_sentences_en_2, tokenizer_en)\n",
    "\n",
    "tokenized_test_sentences_en    = tokenize(test_sentences_en,tokenizer_en)\n",
    "\n",
    "#EN---------------------------\n",
    "tokenized_train_sentences_fr_0 = tokenize(train_sentences_fr_0,tokenizer_fr)\n",
    "tokenized_train_sentences_fr_1 = tokenize(train_sentences_fr_1,tokenizer_fr)\n",
    "tokenized_train_sentences_fr_2 = tokenize(train_sentences_fr_2,tokenizer_fr)\n",
    "\n",
    "tokenized_test_sentences_fr    = tokenize(test_sentences_fr, tokenizer_fr)\n",
    "\n",
    "#IT---------------------------\n",
    "tokenized_train_sentences_it_0 = tokenize(train_sentences_it_0, tokenizer_it)\n",
    "tokenized_train_sentences_it_1 = tokenize(train_sentences_it_1, tokenizer_it)\n",
    "tokenized_train_sentences_it_2 = tokenize(train_sentences_it_2, tokenizer_it)\n",
    "\n",
    "tokenized_test_sentences_it    = tokenize(test_sentences_it, tokenizer_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f02c6d",
   "metadata": {},
   "source": [
    "##### Load embedding vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "146f85da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load embeddings 100 \n",
    "ft_en_100 = fasttext.load_model('pretrained_embeddings/cc.en.100.bin')\n",
    "\n",
    "#Load embeddings 300 \n",
    "ft_en_300 = fasttext.load_model('pretrained_embeddings/cc.en.300.bin')\n",
    "\n",
    "#Load embeddings 100 FR\n",
    "ft_fr_100 = fasttext.load_model('pretrained_embeddings/cc.fr.100.bin')\n",
    "\n",
    "#Load embeddings 300 FR\n",
    "ft_fr_300 = fasttext.load_model('pretrained_embeddings/cc.fr.300.bin')\n",
    "\n",
    "#Load embeddings 100 IT\n",
    "ft_it_100 = fasttext.load_model('pretrained_embeddings/cc.it.100.bin')\n",
    "\n",
    "#Load embeddings 300 IT\n",
    "ft_it_300 = fasttext.load_model('pretrained_embeddings/cc.it.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46702b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27372fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace words in sentences with their corresponding vector\n",
    "def vectorize(sentences, ft):\n",
    "    sentences_vectorised = []\n",
    "    for s in sentences:\n",
    "        sentence_vectorised = []\n",
    "        for word in s:\n",
    "            sentence_vectorised.append(ft.get_word_vector(word))\n",
    "        sentences_vectorised.append(sentence_vectorised)\n",
    "    return sentences_vectorised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbf34a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EN-----------------------------\n",
    "vectorized_train_sentences_en_0 = vectorize(tokenized_train_sentences_en_0, ft_en_100)\n",
    "vectorized_train_sentences_en_1 = vectorize(tokenized_train_sentences_en_1, ft_en_100)\n",
    "vectorized_train_sentences_en_2 = vectorize(tokenized_train_sentences_en_2, ft_en_100)\n",
    "\n",
    "vectorized_test_sentences_en    = vectorize(tokenized_test_sentences_en, ft_en_100)\n",
    "\n",
    "#EN-----------------------------\n",
    "vectorized_train_sentences_fr_0 = vectorize(tokenized_train_sentences_fr_0, ft_fr_100)\n",
    "vectorized_train_sentences_fr_1 = vectorize(tokenized_train_sentences_fr_1, ft_fr_100)\n",
    "vectorized_train_sentences_fr_2 = vectorize(tokenized_train_sentences_fr_2, ft_fr_100)\n",
    "vectorized_test_sentences_fr    = vectorize(tokenized_test_sentences_fr, ft_fr_100)\n",
    "\n",
    "#EN-----------------------------\n",
    "vectorized_train_sentences_it_0 = vectorize(tokenized_train_sentences_it_0, ft_it_100)\n",
    "vectorized_train_sentences_it_1 = vectorize(tokenized_train_sentences_it_1, ft_it_100)\n",
    "vectorized_train_sentences_it_2 = vectorize(tokenized_train_sentences_it_2, ft_it_100)\n",
    "\n",
    "vectorized_test_sentences_it    = vectorize(tokenized_test_sentences_it, ft_it_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b5ab0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding function:\n",
    "#Adapt dataset so that the sentence vectors are all of the same length \n",
    "def pad_vectorized_sentences(sentences, size):\n",
    "    #Find out the dimension of the word vector -- we need to pad the sentence with 0 vectors of this dimension\n",
    "    embedding_length = len(sentences[0][0]) # length first word vector of the first sentence of the dataset\n",
    "    X = []\n",
    "    i =0\n",
    "    for s in sentences:\n",
    "        result = np.zeros((size, embedding_length)) \n",
    "        s = np.array(s)\n",
    "        result[:s.shape[0],:s.shape[1]] = s\n",
    "        X.append(result)\n",
    "        i = i+1\n",
    "    return np.array(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2db67b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 18  # for english 15 was chosen -- better performance\n",
    "#EN----------------\n",
    "X_train_padded_en_0 = pad_vectorized_sentences(vectorized_train_sentences_en_0, 15)\n",
    "X_train_padded_en_1 = pad_vectorized_sentences(vectorized_train_sentences_en_1, 15)\n",
    "X_train_padded_en_2 = pad_vectorized_sentences(vectorized_train_sentences_en_2, 15)\n",
    "\n",
    "X_test_padded_en  = pad_vectorized_sentences(vectorized_test_sentences_en, 15)\n",
    "\n",
    "#FR----------------\n",
    "X_train_padded_fr_0 = pad_vectorized_sentences(vectorized_train_sentences_fr_0, size)\n",
    "X_train_padded_fr_1 = pad_vectorized_sentences(vectorized_train_sentences_fr_1, size)\n",
    "X_train_padded_fr_2 = pad_vectorized_sentences(vectorized_train_sentences_fr_2, size)\n",
    "\n",
    "X_test_padded_fr  = pad_vectorized_sentences(vectorized_test_sentences_fr, size)\n",
    "\n",
    "#IT----------------\n",
    "X_train_padded_it_0 = pad_vectorized_sentences(vectorized_train_sentences_it_0, size)\n",
    "X_train_padded_it_1 = pad_vectorized_sentences(vectorized_train_sentences_it_1, size)\n",
    "X_train_padded_it_2 = pad_vectorized_sentences(vectorized_train_sentences_it_2, size)\n",
    "\n",
    "X_test_padded_it  = pad_vectorized_sentences(vectorized_test_sentences_it, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a848d3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2091a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to tensors (datatype: float32)\n",
    "\n",
    "#EN-------------------------\n",
    "X_train_en_0 = torch.tensor(X_train_padded_en_0.astype('float32'))\n",
    "X_train_en_1 = torch.tensor(X_train_padded_en_1.astype('float32'))\n",
    "X_train_en_2 = torch.tensor(X_train_padded_en_2.astype('float32'))\n",
    "\n",
    "Y_train_en_0   = torch.tensor(train_labels_en_0)\n",
    "Y_train_en_1   = torch.tensor(train_labels_en_1)\n",
    "Y_train_en_2   = torch.tensor(train_labels_en_2)\n",
    "\n",
    "#Testing dataset\n",
    "X_test_en = torch.tensor(X_test_padded_en.astype('float32'))\n",
    "Y_test_en = torch.tensor(test_labels_en)\n",
    "\n",
    "\n",
    "#FR-------------------------\n",
    "X_train_fr_0 = torch.tensor(X_train_padded_fr_0.astype('float32'))\n",
    "X_train_fr_1 = torch.tensor(X_train_padded_fr_1.astype('float32'))\n",
    "X_train_fr_2 = torch.tensor(X_train_padded_fr_2.astype('float32'))\n",
    "\n",
    "Y_train_fr_0   = torch.tensor(train_labels_fr_0)\n",
    "Y_train_fr_1   = torch.tensor(train_labels_fr_1)\n",
    "Y_train_fr_2   = torch.tensor(train_labels_fr_2)\n",
    "\n",
    "#Testing dataset\n",
    "X_test_fr = torch.tensor(X_test_padded_fr.astype('float32'))\n",
    "Y_test_fr = torch.tensor(test_labels_fr)\n",
    "\n",
    "\n",
    "#FR-------------------------\n",
    "X_train_it_0 = torch.tensor(X_train_padded_it_0.astype('float32'))\n",
    "X_train_it_1 = torch.tensor(X_train_padded_it_1.astype('float32'))\n",
    "X_train_it_2 = torch.tensor(X_train_padded_it_2.astype('float32'))\n",
    "\n",
    "Y_train_it_0   = torch.tensor(train_labels_it_0)\n",
    "Y_train_it_1   = torch.tensor(train_labels_it_1)\n",
    "Y_train_it_2   = torch.tensor(train_labels_it_2)\n",
    "\n",
    "#Testing dataset\n",
    "X_test_it = torch.tensor(X_test_padded_it.astype('float32'))\n",
    "Y_test_it = torch.tensor(test_labels_it)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c860033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a974c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataset tensors in a file, for other future usages:\n",
    "\n",
    "#EN----------------------------------\n",
    "torch.save(X_train_en_0,'tensors/x_train_en_0_file.pt') \n",
    "torch.save(X_train_en_1,'tensors/x_train_en_1_file.pt')\n",
    "torch.save(X_train_en_2,'tensors/x_train_en_2_file.pt')\n",
    "torch.save(Y_train_en_0,'tensors/y_train_en_0_file.pt')\n",
    "torch.save(Y_train_en_1,'tensors/y_train_en_1_file.pt')\n",
    "torch.save(Y_train_en_2,'tensors/y_train_en_2_file.pt')\n",
    "\n",
    "#Testing dataset\n",
    "torch.save(X_test_en,'tensors/x_test_en_file.pt')\n",
    "torch.save(Y_test_en,'tensors/y_test_en_file.pt')\n",
    "\n",
    "#EN-----------------------------------\n",
    "torch.save(X_train_fr_0,'tensors/x_train_fr_0_file.pt') \n",
    "torch.save(X_train_fr_1,'tensors/x_train_fr_1_file.pt')\n",
    "torch.save(X_train_fr_2,'tensors/x_train_fr_2_file.pt')\n",
    "torch.save(Y_train_fr_0,'tensors/y_train_fr_0_file.pt')\n",
    "torch.save(Y_train_fr_1,'tensors/y_train_fr_1_file.pt')\n",
    "torch.save(Y_train_fr_2,'tensors/y_train_fr_2_file.pt')\n",
    "\n",
    "#Testing dataset\n",
    "torch.save(X_test_fr,'tensors/x_test_fr_file.pt')\n",
    "torch.save(Y_test_fr,'tensors/y_test_fr_file.pt')\n",
    "\n",
    "#EN-----------------------------------\n",
    "torch.save(X_train_it_0,'tensors/x_train_it_0_file.pt') \n",
    "torch.save(X_train_it_1,'tensors/x_train_it_1_file.pt')\n",
    "torch.save(X_train_it_2,'tensors/x_train_it_2_file.pt')\n",
    "torch.save(Y_train_it_0,'tensors/y_train_it_0_file.pt')\n",
    "torch.save(Y_train_it_1,'tensors/y_train_it_1_file.pt')\n",
    "torch.save(Y_train_it_2,'tensors/y_train_it_2_file.pt')\n",
    "\n",
    "#Testing dataset\n",
    "torch.save(X_test_it,'tensors/x_test_it_file.pt')\n",
    "torch.save(Y_test_it,'tensors/y_test_it_file.pt')\n",
    "\n",
    "#loaded_tesnsor = torch.load('y_test_file.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2373a8a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7f5c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenation: merge all the folds \n",
    "\n",
    "X_train_en = torch.cat((X_train_en_0,X_train_en_1,X_train_en_2),0)\n",
    "X_train_fr = torch.cat((X_train_fr_0,X_train_fr_1,X_train_fr_2),0)\n",
    "X_train_it = torch.cat((X_train_it_0,X_train_it_1,X_train_it_2),0)\n",
    "\n",
    "Y_train_en = torch.cat((Y_train_en_0,Y_train_en_1,Y_train_en_2),0)\n",
    "Y_train_fr = torch.cat((Y_train_fr_0,Y_train_fr_1,Y_train_fr_2),0)\n",
    "Y_train_it = torch.cat((Y_train_it_0,Y_train_it_1,Y_train_it_2),0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f57b045",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "100e9e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, n_layers):\n",
    "       \n",
    "        super(LSTM,self).__init__()\n",
    "        \n",
    "        self.input_size  = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers    = n_layers\n",
    "        \n",
    "        #LSTM layer\n",
    "        self.lstm_layer = nn.LSTM(input_size, hidden_size, n_layers,batch_first=True)\n",
    "        #LINEAR layer\n",
    "        self.output_layer = nn.Linear(hidden_size,1)\n",
    "    \n",
    "    def forward(self, input_sentence, hidden_state ):\n",
    "        \n",
    "        sig = nn.Sigmoid()\n",
    "      \n",
    "        output, state = self.lstm_layer(input_sentence, hidden_state ) \n",
    "     \n",
    "        output_network = sig(self.output_layer(state[0]).view(1))\n",
    "        \n",
    "        return(output_network, state)\n",
    "\n",
    "    def init_hidden(self):\n",
    "      \n",
    "        return (torch.zeros(self.n_layers, 1, self.hidden_size),\n",
    "                torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c1184f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, input_s, label):\n",
    "    hidden, cell = model.init_hidden()\n",
    "    model.zero_grad()\n",
    "    loss = 0\n",
    "    output, (hidden, cell) = model(input_s, (hidden, cell))\n",
    "    loss += criterion(output, label.view(1).to(torch.float32))   \n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56aac450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The train and time logging function were taken from assignment 3 of the course\n",
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac68e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dd1d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and run model: \n",
    "\n",
    "def create_model(X,Y, hidden_size, optimizer_key, n_layers, loss_function, lr, n_epochs):\n",
    "    \n",
    "    input_size = X.size()[2] \n",
    "    seq_len    = X.size()[1] \n",
    "    \n",
    "    model = LSTM(input_size, hidden_size, n_layers)\n",
    "    \n",
    "    criterion = loss_function\n",
    "    \n",
    "    start = time.time()\n",
    "    all_losses = []\n",
    "    loss_avg = 0\n",
    "    optimizer = 0\n",
    "    \n",
    "    if optimizer_key == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    elif optimizer_key == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_key == 'RMSprop':\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    elif optimizer_key == 'Adagrad':\n",
    "        optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)\n",
    "\n",
    "    \n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        for i in range(X.size()[0]): \n",
    "            \n",
    "            loss = train(model,optimizer, X[i].view(1, seq_len,input_size ),Y[i])\n",
    "            loss_avg += loss\n",
    "\n",
    "        print('[{} ({} {}%) {:.4f}]'.format(time_since(start), epoch, epoch/n_epochs * 100, loss))\n",
    "    \n",
    "\n",
    "    all_losses.append(loss_avg/ n_epochs)\n",
    "    loss_avg = 0\n",
    "    return model, all_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f958a639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(model,X_test,Y_test):\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    \n",
    "    hidden, cell = model.init_hidden()\n",
    "    logits  = []\n",
    "    labels  = []\n",
    "    seq_len = X_test.size()[1] \n",
    "    input_size = X_test.size()[2] \n",
    "    for i in range(X_test.size()[0]):\n",
    "        \n",
    "        gg, (hidden,cell) = model(X_test[i].view(1,seq_len,input_size ),(hidden, cell))\n",
    "        \n",
    "        if(gg[0]>=0.5):\n",
    "            logits.append(1) \n",
    "        else:\n",
    "            logits.append(0)   \n",
    "            \n",
    "        labels.append(Y_test[i])\n",
    "    \n",
    "    correct = 0\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(logits)):\n",
    "        labl = labels[i]\n",
    "        pred = logits[i]\n",
    "        if (labl==pred):\n",
    "            if labl == 1:\n",
    "                tp += 1\n",
    "            else: \n",
    "                tn += 1\n",
    "        else: \n",
    "            if labl == 1:\n",
    "                fn += 1\n",
    "            else: \n",
    "                fp += 1\n",
    "            \n",
    "    accuracy = (tp+tn)/len(labels)\n",
    "    precision = (tp)/(tp+fp)\n",
    "    recall = (tp)/(tp+fn)\n",
    "    f1 = 2*tp/(2*tp+fp+fn)\n",
    "    \n",
    "    return (accuracy,precision,recall,f1)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ade04",
   "metadata": {},
   "source": [
    "###  EXPERIMETS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c4eb7",
   "metadata": {},
   "source": [
    "## ENGLISH \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a446e570",
   "metadata": {},
   "source": [
    "### Fold 0, base config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf73d340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 8s (1 12.5%) 0.5796]\n",
      "[0m 16s (2 25.0%) 0.5792]\n",
      "[0m 25s (3 37.5%) 0.5798]\n",
      "[0m 33s (4 50.0%) 0.5862]\n",
      "[0m 41s (5 62.5%) 0.4811]\n",
      "[0m 49s (6 75.0%) 0.1764]\n",
      "[0m 57s (7 87.5%) 0.1657]\n",
      "[1m 5s (8 100.0%) 0.1498]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.7813873626373626\n",
      "Precision: 0.8138898392062949\n",
      "Recall: 0.6942944695753684\n",
      "F1: 0.7493503425466572\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "experiment = \"En 0 base\"\n",
    "n_epochs = 8\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_0\n",
    "Y= Y_train_en_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6487c437",
   "metadata": {},
   "source": [
    "### Fold 1, base config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f232309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 7s (1 100.0%) 0.5686]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.4706730769230769\n",
      "Precision: 0.4706730769230769\n",
      "Recall: 1.0\n",
      "F1: 0.6400784570120954\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "experiment = \"En 1 base\"\n",
    "n_epochs = 1\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_1\n",
    "Y= Y_train_en_1\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0e3a35",
   "metadata": {},
   "source": [
    "### Fold 2, base config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "49a3574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 8s (1 12.5%) 0.7872]\n",
      "[0m 16s (2 25.0%) 0.7848]\n",
      "[0m 24s (3 37.5%) 0.7860]\n",
      "[0m 32s (4 50.0%) 0.7934]\n",
      "[0m 40s (5 62.5%) 0.7390]\n",
      "[0m 48s (6 75.0%) 0.2196]\n",
      "[0m 56s (7 87.5%) 0.2076]\n",
      "[1m 4s (8 100.0%) 0.2255]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.7934065934065934\n",
      "Precision: 0.7940951506807404\n",
      "Recall: 0.7574784765796002\n",
      "F1: 0.7753547423450337\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "experiment = \"En 2 base\"\n",
    "n_epochs = 8\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_2\n",
    "Y= Y_train_en_2\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0612429f",
   "metadata": {},
   "source": [
    "### Fold 0, base config + change embedding vector to 300 features (instead of 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e2c6c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 8s (1 12.5%) 0.5793]\n",
      "[0m 17s (2 25.0%) 0.5797]\n",
      "[0m 26s (3 37.5%) 0.6038]\n",
      "[0m 34s (4 50.0%) 0.2282]\n",
      "[0m 43s (5 62.5%) 0.1755]\n",
      "[0m 52s (6 75.0%) 0.1286]\n",
      "[1m 0s (7 87.5%) 0.1083]\n",
      "[1m 9s (8 100.0%) 0.0952]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.7684752747252748\n",
      "Precision: 0.8003795721187026\n",
      "Recall: 0.6769298117612724\n",
      "F1: 0.7334967191082299\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Prepare input data: \n",
    "vectorized_300_train_sentences_en_0 = vectorize(tokenized_train_sentences_en_0, ft_en_300)\n",
    "vectorized_300_test_sentences_en    = vectorize(tokenized_test_sentences_en, ft_en_300)\n",
    "X_train_padded_en_0_300 = pad_vectorized_sentences(vectorized_300_train_sentences_en_0, 15)\n",
    "X_test_padded_en_300  = pad_vectorized_sentences(vectorized_300_test_sentences_en, 15)\n",
    "\n",
    "experiment = \"EN 0 with 300 ft\"\n",
    "n_epochs = 8\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= torch.tensor(X_train_padded_en_0_300.astype('float32'))\n",
    "Y= Y_train_en_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = torch.tensor(X_test_padded_en_300.astype('float32'))\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf1f8fe",
   "metadata": {},
   "source": [
    "### Fold 1, base config + more epochs   (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4617743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 8s (1 10.0%) 0.5687]\n",
      "[0m 16s (2 20.0%) 0.5689]\n",
      "[0m 24s (3 30.0%) 0.5689]\n",
      "[0m 32s (4 40.0%) 0.5688]\n",
      "[0m 40s (5 50.0%) 0.5648]\n",
      "[0m 48s (6 60.0%) 0.2531]\n",
      "[0m 56s (7 70.0%) 0.1104]\n",
      "[1m 4s (8 80.0%) 0.1307]\n",
      "[1m 12s (9 90.0%) 0.1474]\n",
      "[1m 20s (10 100.0%) 0.1557]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.8116071428571429\n",
      "Precision: 0.8036347517730497\n",
      "Recall: 0.7936670071501533\n",
      "F1: 0.7986197782835328\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#EN Fold 1\n",
    "experiment = \"En fold 1\"\n",
    "n_epochs = 10\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_1\n",
    "Y= Y_train_en_1\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d2774",
   "metadata": {},
   "source": [
    " ### Fold 1, base config + more epochs   (15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37ae7136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 8s (1 6.666666666666667%) 0.5686]\n",
      "[0m 16s (2 13.333333333333334%) 0.5688]\n",
      "[0m 24s (3 20.0%) 0.5688]\n",
      "[0m 31s (4 26.666666666666668%) 0.5683]\n",
      "[0m 39s (5 33.33333333333333%) 0.3843]\n",
      "[0m 47s (6 40.0%) 0.1497]\n",
      "[0m 55s (7 46.666666666666664%) 0.1231]\n",
      "[1m 2s (8 53.333333333333336%) 0.1373]\n",
      "[1m 10s (9 60.0%) 0.1435]\n",
      "[1m 18s (10 66.66666666666666%) 0.1528]\n",
      "[1m 26s (11 73.33333333333333%) 0.1483]\n",
      "[1m 34s (12 80.0%) 0.1506]\n",
      "[1m 42s (13 86.66666666666667%) 0.1452]\n",
      "[1m 49s (14 93.33333333333333%) 0.1416]\n",
      "[1m 57s (15 100.0%) 0.1415]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.7754120879120879\n",
      "Precision: 0.7843199492144104\n",
      "Recall: 0.7211440245148111\n",
      "F1: 0.7514064162992246\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#EN Fold 1\n",
    "experiment = \"En fold 1\"\n",
    "n_epochs = 15\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_1\n",
    "Y= Y_train_en_1\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1c89ad",
   "metadata": {},
   "source": [
    "### Fold 1, base config + more epochs   (20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7ec5430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 7s (1 5.0%) 0.5688]\n",
      "[0m 15s (2 10.0%) 0.5690]\n",
      "[0m 23s (3 15.0%) 0.5691]\n",
      "[0m 31s (4 20.0%) 0.5690]\n",
      "[0m 39s (5 25.0%) 0.4241]\n",
      "[0m 47s (6 30.0%) 0.1575]\n",
      "[0m 55s (7 35.0%) 0.1220]\n",
      "[1m 3s (8 40.0%) 0.1513]\n",
      "[1m 11s (9 45.0%) 0.1530]\n",
      "[1m 19s (10 50.0%) 0.1619]\n",
      "[1m 27s (11 55.00000000000001%) 0.1589]\n",
      "[1m 35s (12 60.0%) 0.1624]\n",
      "[1m 43s (13 65.0%) 0.1631]\n",
      "[1m 50s (14 70.0%) 0.1636]\n",
      "[1m 58s (15 75.0%) 0.1648]\n",
      "[2m 6s (16 80.0%) 0.1626]\n",
      "[2m 14s (17 85.0%) 0.1679]\n",
      "[2m 22s (18 90.0%) 0.1662]\n",
      "[2m 30s (19 95.0%) 0.1672]\n",
      "[2m 38s (20 100.0%) 0.1703]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.8022664835164836\n",
      "Precision: 0.8228794280142997\n",
      "Recall: 0.7389464468116154\n",
      "F1: 0.7786576458829861\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#EN Fold 1\n",
    "experiment = \"En fold 1\"\n",
    "n_epochs = 20\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_1\n",
    "Y= Y_train_en_1\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8fb5cf",
   "metadata": {},
   "source": [
    "### Fold 0, base config + num hidden size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5caa1",
   "metadata": {},
   "source": [
    "### Fold 0, base config + learning rate  (0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad7a9946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 8s (1 12.5%) 0.6067]\n",
      "[0m 16s (2 25.0%) 0.6083]\n",
      "[0m 23s (3 37.5%) 0.5791]\n",
      "[0m 31s (4 50.0%) 0.1330]\n",
      "[0m 39s (5 62.5%) 0.1486]\n",
      "[0m 47s (6 75.0%) 0.0972]\n",
      "[0m 55s (7 87.5%) 0.0979]\n",
      "[1m 2s (8 100.0%) 0.0882]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.7684752747252748\n",
      "Precision: 0.8072714436992587\n",
      "Recall: 0.6674449146359259\n",
      "F1: 0.730729291476955\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#EN Fold 0\n",
    "experiment = \"En fold 1\"\n",
    "n_epochs = 8\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "lr = 0.2\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_0\n",
    "Y= Y_train_en_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4ee955",
   "metadata": {},
   "source": [
    "###  Fold 0, base config + learning rate  (0.2) + n epoch = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c516762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 7s (1 6.666666666666667%) 0.6065]\n",
      "[0m 15s (2 13.333333333333334%) 0.6068]\n",
      "[0m 23s (3 20.0%) 0.6092]\n",
      "[0m 31s (4 26.666666666666668%) 0.4198]\n",
      "[0m 39s (5 33.33333333333333%) 0.2308]\n",
      "[0m 47s (6 40.0%) 0.1236]\n",
      "[0m 55s (7 46.666666666666664%) 0.1088]\n",
      "[1m 3s (8 53.333333333333336%) 0.0947]\n",
      "[1m 10s (9 60.0%) 0.0885]\n",
      "[1m 18s (10 66.66666666666666%) 0.0833]\n",
      "[1m 26s (11 73.33333333333333%) 0.0818]\n",
      "[1m 34s (12 80.0%) 0.0796]\n",
      "[1m 42s (13 86.66666666666667%) 0.0882]\n",
      "[1m 50s (14 93.33333333333333%) 0.0863]\n",
      "[1m 58s (15 100.0%) 0.1296]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.7513736263736264\n",
      "Precision: 0.8299652990406206\n",
      "Recall: 0.5933167955639865\n",
      "F1: 0.6919673247106876\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#EN Fold 0\n",
    "experiment = \"En fold 1\"\n",
    "n_epochs = 15\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "lr = 0.2\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_0\n",
    "Y= Y_train_en_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6b02d2",
   "metadata": {},
   "source": [
    "###  Fold 0, base config + learning rate  (0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71b0f173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 7s (1 12.5%) 0.6303]\n",
      "[0m 15s (2 25.0%) 0.6303]\n",
      "[0m 23s (3 37.5%) 0.6305]\n",
      "[0m 31s (4 50.0%) 0.6313]\n",
      "[0m 39s (5 62.5%) 0.6388]\n",
      "[0m 46s (6 75.0%) 0.1385]\n",
      "[0m 54s (7 87.5%) 0.1730]\n",
      "[1m 2s (8 100.0%) 0.1401]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.724793956043956\n",
      "Precision: 0.8117879053461875\n",
      "Recall: 0.5406391361447541\n",
      "F1: 0.6490321450468599\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#EN Fold 0\n",
    "experiment = \"En fold 1\"\n",
    "n_epochs = 8\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "lr = 0.3\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_0\n",
    "Y= Y_train_en_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c389de3",
   "metadata": {},
   "source": [
    "### More padding: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7eddc1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 10s (1 12.5%) 0.5790]\n",
      "[0m 20s (2 25.0%) 0.5785]\n",
      "[0m 30s (3 37.5%) 0.5784]\n",
      "[0m 40s (4 50.0%) 0.5784]\n",
      "[0m 50s (5 62.5%) 0.5784]\n",
      "[1m 0s (6 75.0%) 0.5784]\n",
      "[1m 10s (7 87.5%) 0.5784]\n",
      "[1m 21s (8 100.0%) 0.5784]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-957560cdb166>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precision: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-bbd72bb94840>\u001b[0m in \u001b[0;36mmetrics\u001b[0;34m(model, X_test, Y_test)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "X_train_padded_en_0_18 = pad_vectorized_sentences(vectorized_train_sentences_en_0, 20)\n",
    "X_test_padded_en_18  = pad_vectorized_sentences(vectorized_test_sentences_en, 20)\n",
    "#EN Fold 0\n",
    "experiment = \"En fold 1 more padding\"\n",
    "n_epochs = 8\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= torch.tensor(X_train_padded_en_0_18.astype('float32'))\n",
    "Y= Y_train_en_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = torch.tensor(X_test_padded_en_18.astype('float32'))\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dafe00",
   "metadata": {},
   "source": [
    "### Fold 0 base with different optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c3db1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 9s (1 12.5%) 0.1440]\n",
      "[0m 19s (2 25.0%) 0.1116]\n",
      "[0m 28s (3 37.5%) 0.1084]\n",
      "[0m 38s (4 50.0%) 0.1021]\n",
      "[0m 47s (5 62.5%) 0.1057]\n",
      "[0m 57s (6 75.0%) 0.1035]\n",
      "[1m 6s (7 87.5%) 0.1022]\n",
      "[1m 16s (8 100.0%) 0.1010]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.5293269230769231\n",
      "Precision: 0.5\n",
      "Recall: 0.00014592149423610097\n",
      "F1: 0.00029175784099197665\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "experiment = \"en fold 0 adam\"\n",
    "n_epochs = 8\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "lr = 0.01\n",
    "optimizer = 'Adam'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_0\n",
    "Y= Y_train_en_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n optimizer: \"+ optimizer\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a65ae",
   "metadata": {},
   "source": [
    "## Try out random subsets per epoch: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f849b595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up batches\n",
    "# importing the required libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import seaborn as sns\n",
    "from torch.utils.data import TensorDataset\n",
    "import random\n",
    "\n",
    "dataset = TensorDataset(X_train_en_0, Y_train_en_0)\n",
    "  \n",
    "# implementing dataloader on the dataset \n",
    "# and printing per batch\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=200, \n",
    "                        shuffle=False)\n",
    "\n",
    "batches = []\n",
    "for batch in dataloader:\n",
    "    batches.append(batch)\n",
    "    \n",
    "\n",
    "length = len(batches)\n",
    "random_int = random.randint(0, length)\n",
    "batch_size = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18672037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 0s (1 1.0%) 0.4666]\n",
      "[0m 1s (2 2.0%) 0.5085]\n",
      "[0m 2s (3 3.0%) 0.6929]\n",
      "[0m 3s (4 4.0%) 0.6935]\n",
      "[0m 4s (5 5.0%) 0.7945]\n",
      "[0m 5s (6 6.0%) 0.6525]\n",
      "[0m 6s (7 7.000000000000001%) 0.4757]\n",
      "[0m 7s (8 8.0%) 0.5203]\n",
      "[0m 7s (9 9.0%) 0.5188]\n",
      "[0m 8s (10 10.0%) 0.6557]\n",
      "[0m 9s (11 11.0%) 0.6910]\n",
      "[0m 10s (12 12.0%) 0.6905]\n",
      "[0m 11s (13 13.0%) 0.7950]\n",
      "[0m 12s (14 14.000000000000002%) 0.8296]\n",
      "[0m 13s (15 15.0%) 0.5212]\n",
      "[0m 14s (16 16.0%) 0.4788]\n",
      "[0m 15s (17 17.0%) 0.8773]\n",
      "[0m 16s (18 18.0%) 0.6541]\n",
      "[0m 17s (19 19.0%) 0.6912]\n",
      "[0m 17s (20 20.0%) 0.8282]\n",
      "[0m 18s (21 21.0%) 0.6919]\n",
      "[0m 19s (22 22.0%) 0.6340]\n",
      "[0m 20s (23 23.0%) 0.6329]\n",
      "[0m 21s (24 24.0%) 0.8263]\n",
      "[0m 22s (25 25.0%) 0.8251]\n",
      "[0m 23s (26 26.0%) 0.8743]\n",
      "[0m 24s (27 27.0%) 0.8741]\n",
      "[0m 25s (28 28.000000000000004%) 0.6509]\n",
      "[0m 26s (29 28.999999999999996%) 0.8726]\n",
      "[0m 27s (30 30.0%) 0.6503]\n",
      "[0m 27s (31 31.0%) 0.6909]\n",
      "[0m 28s (32 32.0%) 0.7985]\n",
      "[0m 29s (33 33.0%) 0.6134]\n",
      "[0m 30s (34 34.0%) 0.7074]\n",
      "[0m 31s (35 35.0%) 0.5683]\n",
      "[0m 32s (36 36.0%) 0.6279]\n",
      "[0m 33s (37 37.0%) 0.8172]\n",
      "[0m 34s (38 38.0%) 0.5344]\n",
      "[0m 34s (39 39.0%) 0.5836]\n",
      "[0m 35s (40 40.0%) 0.5721]\n",
      "[0m 36s (41 41.0%) 0.1705]\n",
      "[0m 37s (42 42.0%) 0.5789]\n",
      "[0m 38s (43 43.0%) 0.3387]\n",
      "[0m 39s (44 44.0%) 0.2694]\n",
      "[0m 40s (45 45.0%) 0.2001]\n",
      "[0m 41s (46 46.0%) 0.1891]\n",
      "[0m 41s (47 47.0%) 0.0708]\n",
      "[0m 42s (48 48.0%) 0.3665]\n",
      "[0m 43s (49 49.0%) 0.1744]\n",
      "[0m 44s (50 50.0%) 0.0702]\n",
      "[0m 45s (51 51.0%) 0.0940]\n",
      "[0m 46s (52 52.0%) 0.1017]\n",
      "[0m 47s (53 53.0%) 0.1654]\n",
      "[0m 47s (54 54.0%) 0.1197]\n",
      "[0m 48s (55 55.00000000000001%) 0.1175]\n",
      "[0m 49s (56 56.00000000000001%) 0.1715]\n",
      "[0m 50s (57 56.99999999999999%) 0.0747]\n",
      "[0m 51s (58 57.99999999999999%) 0.1619]\n",
      "[0m 52s (59 59.0%) 0.1552]\n",
      "[0m 52s (60 60.0%) 0.0695]\n",
      "[0m 53s (61 61.0%) 0.1073]\n",
      "[0m 54s (62 62.0%) 0.1657]\n",
      "[0m 55s (63 63.0%) 0.1230]\n",
      "[0m 56s (64 64.0%) 1.4671]\n",
      "[0m 57s (65 65.0%) 0.1503]\n",
      "[0m 57s (66 66.0%) 0.0869]\n",
      "[0m 58s (67 67.0%) 0.0644]\n",
      "[0m 59s (68 68.0%) 0.0908]\n",
      "[1m 0s (69 69.0%) 0.0889]\n",
      "[1m 1s (70 70.0%) 1.6002]\n",
      "[1m 2s (71 71.0%) 0.0556]\n",
      "[1m 2s (72 72.0%) 0.0404]\n",
      "[1m 3s (73 73.0%) 0.0397]\n",
      "[1m 4s (74 74.0%) 1.4749]\n",
      "[1m 5s (75 75.0%) 0.1173]\n",
      "[1m 6s (76 76.0%) 0.1202]\n",
      "[1m 7s (77 77.0%) 1.6107]\n",
      "[1m 8s (78 78.0%) 0.0520]\n",
      "[1m 8s (79 79.0%) 1.4204]\n",
      "[1m 9s (80 80.0%) 0.1257]\n",
      "[1m 10s (81 81.0%) 0.1106]\n",
      "[1m 11s (82 82.0%) 0.1160]\n",
      "[1m 12s (83 83.0%) 0.0416]\n",
      "[1m 13s (84 84.0%) 0.1905]\n",
      "[1m 14s (85 85.0%) 0.0971]\n",
      "[1m 15s (86 86.0%) 0.1219]\n",
      "[1m 15s (87 87.0%) 0.0404]\n",
      "[1m 16s (88 88.0%) 0.1039]\n",
      "[1m 17s (89 89.0%) 0.0599]\n",
      "[1m 18s (90 90.0%) 1.3453]\n",
      "[1m 19s (91 91.0%) 0.0862]\n",
      "[1m 20s (92 92.0%) 0.0531]\n",
      "[1m 21s (93 93.0%) 0.1307]\n",
      "[1m 22s (94 94.0%) 0.0858]\n",
      "[1m 22s (95 95.0%) 0.0866]\n",
      "[1m 23s (96 96.0%) 0.0560]\n",
      "[1m 24s (97 97.0%) 0.0882]\n",
      "[1m 25s (98 98.0%) 0.0550]\n",
      "[1m 26s (99 99.0%) 0.0671]\n",
      "[1m 27s (100 100.0%) 0.0390]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.5293269230769231\n",
      "Precision: 0.5\n",
      "Recall: 0.00014592149423610097\n",
      "F1: 0.00029175784099197665\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#EN Fold 0 WITH random batches per epoch (individual examples are still fed to the network, they are just taken from random \n",
    "#limited in size, subsets over multiple epochs)\n",
    "experiment = \"en_batches_0\"\n",
    "n_epochs = 100\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer_key = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en_0\n",
    "Y= Y_train_en_0\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "\n",
    "input_size = X.size()[2] \n",
    "seq_len    = X.size()[1] \n",
    "\n",
    "model = LSTM(input_size, hidden_size, n_layers)\n",
    "\n",
    "#criterion = loss_function\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "optimizer = 0\n",
    "\n",
    "if optimizer_key == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    random_int = random.randint(0, length-2)\n",
    "    for i in range(batch_size): \n",
    "        \n",
    "        loss = train(model,optimizer, batches[random_int][0][i].view(1, 15,100 ),batches[random_int][1][i])\n",
    "        loss_avg += loss\n",
    "    print('[{} ({} {}%) {:.4f}]'.format(time_since(start), epoch, epoch/n_epochs * 100, loss))\n",
    "\n",
    "\n",
    "all_losses.append(loss_avg/ n_epochs)\n",
    "loss_avg = 0\n",
    "\n",
    "\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626f38bf",
   "metadata": {},
   "source": [
    "### Whole dataset (folds merged):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6e09c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 24s (1 12.5%) 0.7860]\n",
      "[0m 48s (2 25.0%) 0.2183]\n",
      "[1m 12s (3 37.5%) 0.2146]\n",
      "[1m 37s (4 50.0%) 0.2283]\n",
      "[2m 1s (5 62.5%) 0.2542]\n",
      "[2m 26s (6 75.0%) 0.2712]\n",
      "[2m 51s (7 87.5%) 0.2634]\n",
      "[3m 16s (8 100.0%) 0.2775]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.7790521978021978\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "experiment = \"merged en\"\n",
    "n_epochs = 8\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "lr = 0.1\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_en\n",
    "Y= Y_train_en\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_en\n",
    "Y_test = Y_test_en\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8cae97",
   "metadata": {},
   "source": [
    "\n",
    "## FRENCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0d5b1c",
   "metadata": {},
   "source": [
    "###  Fold 1, base config \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd02608f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... \n",
      "[0m 9s (1 10.0%) 0.8300]\n",
      "[0m 18s (2 20.0%) 0.8301]\n",
      "[0m 28s (3 30.0%) 0.8300]\n",
      "[0m 37s (4 40.0%) 0.8300]\n",
      "[0m 46s (5 50.0%) 0.8298]\n",
      "[0m 56s (6 60.0%) 0.8296]\n",
      "[1m 5s (7 70.0%) 0.8291]\n",
      "[1m 14s (8 80.0%) 0.8277]\n",
      "[1m 24s (9 90.0%) 0.8168]\n",
      "[1m 33s (10 100.0%) 0.6310]\n",
      "Computing minumim loss...\n",
      "Computing accuracy...\n",
      "Accuracy: 0.6067994505494505\n",
      "Precision: 0.5540851553509781\n",
      "Recall: 0.8431343936961915\n",
      "F1: 0.6687113014293155\n",
      "Logging experiment...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "experiment = \"FR 1 new base config\"\n",
    "n_epochs = 10\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "lr = 0.15\n",
    "optimizer = 'SGD'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "X= X_train_fr_1\n",
    "Y= Y_train_fr_1\n",
    "\n",
    "\n",
    "print(\"Training model... \")\n",
    "m,losses = create_model(X,Y, hidden_size, optimizer, n_layers, criterion, lr, n_epochs)\n",
    "print(\"Computing minumim loss...\")\n",
    "min_loss = min(losses)\n",
    "\n",
    "print(\"Computing accuracy...\")\n",
    "X_test = X_test_fr\n",
    "Y_test = Y_test_fr\n",
    "\n",
    "\n",
    "a,p,r,f1 = metrics(m,X_test,Y_test)\n",
    "print(\"Accuracy: \"+ str(a))\n",
    "print(\"Precision: \"+ str(p))\n",
    "print(\"Recall: \"+ str(r))\n",
    "print(\"F1: \"+ str(f1))\n",
    "\n",
    "print(\"Logging experiment...\")\n",
    "#Log experiment:\n",
    "file = open('experiments.txt','a')\n",
    "file.write(\"\\n\\nEXPERIMENT \"+ str(experiment)\n",
    "           +\"\\n n_layers: \" + str(n_layers)\n",
    "           +\"\\n hidden_size: \"+ str(hidden_size)\n",
    "           +\"\\n learning_rate: \"+str(lr)\n",
    "           +\"\\n epochs: \"+str(n_epochs)+\"\\n\")\n",
    "file.write(\"Minimum loss:\" + str(min_loss)+\"\\n\")\n",
    "file.write(\"Accuracy :\" + str(a)+\"\\n\")\n",
    "file.close()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
